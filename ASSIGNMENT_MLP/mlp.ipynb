{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessarey libraries\n",
    "import numpy as mp\n",
    "import pandas as da\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,precision_score\n",
    "\n",
    "#dataset features determine if the ground is rock or mine(binary format)\n",
    "df = pd.read_csv('sonarnew.csv')\n",
    "X = df.drop(['Class'], axis='columns')\n",
    "y = df.Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting of data to train and test 80 to 20 percent respectively\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrays to matrices for easier numerical computations\n",
    "X_train = mp.asmatrix(pd.DataFrame(X_train), dtype = 'float64')\n",
    "y_train = mp.asmatrix(pd.DataFrame(y_train), dtype = 'float64')\n",
    "X_test = mp.asmatrix(pd.DataFrame(X_test), dtype = 'float64')\n",
    "y_test = mp.array(pd.DataFrame(y_test), dtype = 'float64')\n",
    "\n",
    "\n",
    "\n",
    "training_inputs = X_train\n",
    "labels = y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preceptron class for training and testing\n",
    "class Neuron(object):\n",
    "\n",
    "    def __init__(self, no_of_feature, max_epoch=100, learning_rate=0.01):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight = mp.zeros(no_of_feature + 1)\n",
    "        print(self.weight.shape)\n",
    "        \n",
    "#prediction fucntion  \n",
    "\n",
    "    def prediction(self, inputs):\n",
    "        summ = mp.dot(inputs, self.weight[1:]) + self.weight[0]\n",
    "        d=self.step(summ)\n",
    "        return d\n",
    "        \n",
    "#activation fucntion is a step function \n",
    "\n",
    "    def step(self,summ):\n",
    "        if summ > 0:\n",
    "            activation= 1\n",
    "        else:\n",
    "            activation = 0\n",
    "        return activation\n",
    "    \n",
    "#training fucntion\n",
    "\n",
    "    def train(self, training_inputs, labels):\n",
    "        for i in range(self.max_epoch):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                predict = self.prediction(inputs)\n",
    "                self.weight[1:] += mp.array(self.learning_rate * (label[0] - predict) * mp.array(inputs)[0])[0]\n",
    "                self.weight[0] += mp.array(self.learning_rate * (label[0] - predict))[0]\n",
    "            print(\"Epoch no\\t\"+str(i+1),\"\\tLearning rate\\t\"+str(self.learning_rate),\"\\nNew weight\"+str(self.weight)) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61,)\n",
      "Epoch no\t1 \tLearning rate\t0.01 \n",
      "New weight[ 0.0000e+00  4.4260e-03  5.3980e-03  2.4790e-03  8.1550e-03  6.5660e-03\n",
      " -2.4240e-03 -3.2150e-03  5.2280e-03  2.4882e-02  3.1254e-02  3.0386e-02\n",
      "  1.9182e-02  6.8930e-03 -1.4980e-03  1.7200e-03  4.7400e-03 -2.7730e-03\n",
      " -1.1271e-02 -7.5330e-03  5.4580e-03  1.2076e-02  6.5140e-03 -3.2600e-03\n",
      " -3.1070e-03 -7.4230e-03 -7.4680e-03 -7.5100e-03  7.3330e-03 -4.6040e-03\n",
      "  7.1900e-04 -7.6750e-03  7.3650e-03 -5.6870e-03 -1.2996e-02 -8.8230e-03\n",
      " -9.9300e-03 -8.2820e-03  3.8080e-03  9.5820e-03 -3.5000e-04  9.6260e-03\n",
      "  2.0759e-02  2.3417e-02  2.8896e-02  3.2352e-02  2.5403e-02  1.5880e-02\n",
      "  1.5566e-02  9.0140e-03  1.6190e-03  1.3840e-03  1.6570e-03  1.3590e-03\n",
      "  1.4240e-03  3.3500e-04  4.3800e-04  8.5000e-05  1.3190e-03  5.9900e-04\n",
      "  2.9700e-04]\n",
      "Epoch no\t2 \tLearning rate\t0.01 \n",
      "New weight[ 0.0000e+00  5.8930e-03  7.5720e-03  2.6400e-03  1.3272e-02  1.0467e-02\n",
      " -4.2910e-03 -7.0670e-03  6.0650e-03  4.0305e-02  4.9027e-02  4.4226e-02\n",
      "  2.6826e-02  4.4530e-03 -7.6970e-03 -7.6570e-03 -5.4390e-03 -7.4000e-03\n",
      " -1.7571e-02 -8.3560e-03  1.3480e-02  2.4555e-02  1.3949e-02  7.4460e-03\n",
      "  1.2279e-02 -3.6410e-03 -1.3005e-02 -8.8020e-03  1.5429e-02 -4.7880e-03\n",
      "  1.3560e-03 -1.9476e-02  1.1511e-02 -7.4640e-03 -1.1157e-02 -1.1563e-02\n",
      " -2.1649e-02 -2.2181e-02  4.4150e-03  1.3040e-02 -4.2010e-03  1.1566e-02\n",
      "  2.4030e-02  3.3827e-02  4.2523e-02  4.5152e-02  3.4891e-02  2.4834e-02\n",
      "  2.6869e-02  1.5865e-02  2.7500e-03  2.7420e-03  2.5600e-03  2.4630e-03\n",
      "  1.9940e-03  1.6900e-04  1.0840e-03 -9.0000e-06  2.1390e-03  1.1820e-03\n",
      "  7.7900e-04]\n",
      "Epoch no\t3 \tLearning rate\t0.01 \n",
      "New weight[-0.01      0.007783  0.009607  0.002684  0.019119  0.015324 -0.005049\n",
      " -0.009905  0.008148  0.051935  0.059346  0.056734  0.03476   0.002707\n",
      " -0.015307 -0.010376 -0.004248 -0.002689 -0.017703 -0.008079  0.011678\n",
      "  0.022734  0.004126 -0.001534  0.009785 -0.012074 -0.026036 -0.021847\n",
      "  0.017981 -0.005603 -0.007117 -0.035395  0.0079   -0.017497 -0.021146\n",
      " -0.016205 -0.028244 -0.031198 -0.001022  0.01567  -0.008012  0.005598\n",
      "  0.020706  0.036793  0.049612  0.050455  0.036479  0.028432  0.034236\n",
      "  0.020567  0.002835  0.003803  0.003517  0.003544  0.002801 -0.000374\n",
      "  0.001301 -0.000419  0.002488  0.00148   0.000979]\n",
      "Epoch no\t4 \tLearning rate\t0.01 \n",
      "New weight[-0.02      0.00898   0.009391  0.002484  0.024919  0.020173 -0.006838\n",
      " -0.014642  0.005926  0.054854  0.060627  0.062209  0.03761  -0.000834\n",
      " -0.025901 -0.020394 -0.011562  0.003227 -0.006547  0.003024  0.010246\n",
      "  0.015982 -0.003003  0.001953  0.020727 -0.013767 -0.034618 -0.024565\n",
      "  0.019952  0.003224  0.004617 -0.039798  0.008342 -0.017589 -0.024764\n",
      " -0.020067 -0.03321  -0.043398 -0.008334  0.01163  -0.018508  0.002156\n",
      "  0.01854   0.038807  0.053587  0.052695  0.035929  0.029065  0.038202\n",
      "  0.023476  0.002898  0.004463  0.004525  0.004059  0.003397 -0.000879\n",
      "  0.001416 -0.000872  0.00288   0.001702  0.001049]\n",
      "Epoch no\t5 \tLearning rate\t0.01 \n",
      "New weight[-2.0000e-02  1.0462e-02  9.8300e-03  1.7330e-03  3.0248e-02  2.5504e-02\n",
      " -1.4007e-02 -2.1303e-02  4.0470e-03  6.1781e-02  7.0515e-02  7.0763e-02\n",
      "  3.8931e-02 -4.7220e-03 -3.2617e-02 -3.0403e-02 -2.1285e-02 -5.8850e-03\n",
      " -1.1110e-02  3.4550e-03  1.3776e-02  2.2180e-02  5.7700e-04 -1.9090e-03\n",
      "  1.9247e-02 -6.2360e-03 -2.0469e-02 -1.2173e-02  2.8598e-02 -3.7000e-05\n",
      "  1.8470e-03 -5.2495e-02  9.5990e-03 -1.4836e-02 -2.7539e-02 -1.6082e-02\n",
      " -2.6026e-02 -3.9733e-02  5.0860e-03  3.4362e-02 -1.2205e-02  1.1914e-02\n",
      "  2.9739e-02  4.8373e-02  6.3727e-02  6.8951e-02  4.9833e-02  3.8610e-02\n",
      "  4.7391e-02  2.8387e-02  4.1780e-03  5.7690e-03  5.4200e-03  4.9640e-03\n",
      "  4.6140e-03 -6.6900e-04  1.7850e-03 -7.0100e-04  3.6770e-03  1.9600e-03\n",
      "  1.3420e-03]\n",
      "Epoch no\t6 \tLearning rate\t0.01 \n",
      "New weight[-2.0000e-02  1.0681e-02  9.9650e-03 -2.7400e-04  3.3796e-02  2.6563e-02\n",
      " -2.0406e-02 -2.7260e-02  1.2000e-03  6.4183e-02  7.0591e-02  7.3207e-02\n",
      "  3.5763e-02 -6.8290e-03 -3.2665e-02 -2.9526e-02 -1.6316e-02 -8.9000e-04\n",
      " -4.1960e-03  1.9798e-02  2.8382e-02  3.2477e-02 -2.6920e-03 -1.5200e-04\n",
      "  2.8073e-02 -7.4470e-03 -2.2518e-02 -1.4353e-02  3.0872e-02 -1.4000e-04\n",
      "  3.6170e-03 -7.1437e-02 -2.1180e-03 -1.4754e-02 -2.2809e-02 -1.3635e-02\n",
      " -2.8759e-02 -5.1659e-02  9.0000e-06  3.6334e-02 -1.7931e-02  1.0444e-02\n",
      "  2.9157e-02  5.1251e-02  6.4128e-02  6.7617e-02  4.9719e-02  4.0728e-02\n",
      "  5.4181e-02  3.2549e-02  4.4420e-03  7.1670e-03  6.3750e-03  5.1780e-03\n",
      "  5.2580e-03 -1.0260e-03  2.2050e-03 -1.0510e-03  4.0350e-03  2.4410e-03\n",
      "  1.5980e-03]\n",
      "Epoch no\t7 \tLearning rate\t0.01 \n",
      "New weight[-0.02      0.013032  0.012023 -0.002091  0.036465  0.02656  -0.024761\n",
      " -0.029557 -0.000727  0.066949  0.07441   0.079967  0.037317 -0.008766\n",
      " -0.033465 -0.022778 -0.008378  0.000667 -0.001397  0.020977  0.021198\n",
      "  0.026647 -0.008559 -0.006531  0.029835 -0.00969  -0.030349 -0.016782\n",
      "  0.036713  0.010478  0.018666 -0.070695  0.003266 -0.007358 -0.027693\n",
      " -0.019257 -0.029431 -0.056267 -0.000273  0.037433 -0.022532  0.015939\n",
      "  0.031714  0.051968  0.066685  0.071132  0.054833  0.047486  0.061616\n",
      "  0.037853  0.005677  0.008246  0.007406  0.005174  0.005728 -0.001182\n",
      "  0.00192  -0.001343  0.004594  0.00262   0.0019  ]\n",
      "Epoch no\t8 \tLearning rate\t0.01 \n",
      "New weight[-0.02      0.013431  0.014137 -0.002011  0.039146  0.027868 -0.026445\n",
      " -0.034055 -0.001934  0.070285  0.077333  0.085109  0.039211 -0.010771\n",
      " -0.03633  -0.026554 -0.009318  0.003426  0.000693  0.015557  0.014043\n",
      "  0.026519 -0.010732 -0.011884  0.032326 -0.00739  -0.02989  -0.018915\n",
      "  0.041164  0.013782  0.019889 -0.084462 -0.004964 -0.010881 -0.029471\n",
      " -0.019037 -0.027724 -0.05736   0.000267  0.0332   -0.030642  0.018962\n",
      "  0.034621  0.058382  0.073697  0.076754  0.056708  0.052673  0.071629\n",
      "  0.043755  0.005827  0.009669  0.00857   0.005474  0.005978 -0.001769\n",
      "  0.002036 -0.001771  0.005009  0.002845  0.002154]\n",
      "Epoch no\t9 \tLearning rate\t0.01 \n",
      "New weight[-0.03      0.014535  0.014867 -0.001812  0.04413   0.030324 -0.032998\n",
      " -0.041141 -0.006801  0.069068  0.077463  0.091631  0.043998 -0.013467\n",
      " -0.047097 -0.033489 -0.015988 -0.002482 -0.003081  0.015958  0.008914\n",
      "  0.022726 -0.013531 -0.017072  0.03887  -0.001983 -0.032404 -0.019445\n",
      "  0.041954  0.011121  0.022226 -0.086039  0.006654 -0.006779 -0.031732\n",
      " -0.017272 -0.022462 -0.058732 -0.001894  0.027969 -0.041787  0.021297\n",
      "  0.037155  0.06156   0.080359  0.084814  0.062679  0.059187  0.078151\n",
      "  0.04776   0.00601   0.010807  0.009412  0.005698  0.006749 -0.002196\n",
      "  0.002046 -0.002185  0.005247  0.003076  0.002261]\n",
      "Epoch no\t10 \tLearning rate\t0.01 \n",
      "New weight[-0.04      0.015574  0.01608  -0.002135  0.047894  0.035027 -0.032693\n",
      " -0.041558 -0.00701   0.070393  0.079019  0.094809  0.042404 -0.018008\n",
      " -0.048621 -0.030911 -0.0112    0.002981 -0.001748  0.022745  0.014624\n",
      "  0.019435 -0.014561 -0.009638  0.042934 -0.013839 -0.046236 -0.026311\n",
      "  0.040226  0.00618   0.017114 -0.098264  0.005639 -0.006159 -0.033072\n",
      " -0.016022 -0.02361  -0.064524 -0.000829  0.039173 -0.037582  0.020272\n",
      "  0.034077  0.059392  0.079822  0.084674  0.064546  0.064441  0.083621\n",
      "  0.051299  0.006691  0.011991  0.01039   0.006506  0.007592 -0.002053\n",
      "  0.002441 -0.002124  0.006084  0.003851  0.002712]\n",
      "Epoch no\t11 \tLearning rate\t0.01 \n",
      "New weight[-0.04      0.016693  0.017416 -0.003564  0.049813  0.035118 -0.035556\n",
      " -0.041612 -0.00583   0.075897  0.082959  0.096288  0.035725 -0.020637\n",
      " -0.053019 -0.028732 -0.00313   0.011917  0.004033  0.030398  0.020932\n",
      "  0.022406 -0.018852 -0.016482  0.045279 -0.010423 -0.048307 -0.028174\n",
      "  0.045098  0.006248  0.025693 -0.100202  0.013406  0.004995 -0.0307\n",
      " -0.0086   -0.012893 -0.061967 -0.001031  0.044262 -0.035033  0.023992\n",
      "  0.03107   0.054903  0.082692  0.087482  0.065079  0.067433  0.090616\n",
      "  0.055699  0.007308  0.01325   0.011163  0.006803  0.008442 -0.002197\n",
      "  0.002564 -0.00224   0.006202  0.004295  0.002982]\n",
      "Epoch no\t12 \tLearning rate\t0.01 \n",
      "New weight[-0.05      0.018369  0.019338 -0.002828  0.056065  0.040575 -0.037394\n",
      " -0.044828 -0.008088  0.076547  0.083573  0.10082   0.036851 -0.025909\n",
      " -0.064167 -0.036391 -0.005919  0.017732  0.003841  0.030692  0.01815\n",
      "  0.020668 -0.016753 -0.009865  0.059645  0.00267  -0.045337 -0.032399\n",
      "  0.042648 -0.005651  0.012449 -0.10864   0.022327  0.002364 -0.035906\n",
      " -0.004233 -0.012448 -0.068776 -0.00815   0.042416 -0.046527  0.019724\n",
      "  0.030397  0.058847  0.089266  0.092289  0.066351  0.07053   0.095406\n",
      "  0.059102  0.007145  0.014085  0.011319  0.007466  0.009253 -0.002361\n",
      "  0.00306  -0.001961  0.006883  0.005029  0.003259]\n",
      "Epoch no\t13 \tLearning rate\t0.01 \n",
      "New weight[-0.06      0.018541  0.019763 -0.003864  0.059275  0.041076 -0.040942\n",
      " -0.051357 -0.013354  0.07609   0.082747  0.101656  0.032291 -0.028625\n",
      " -0.067202 -0.038763 -0.007451  0.017245  0.004501  0.032975  0.012696\n",
      "  0.011837 -0.030849 -0.020654  0.054884 -0.013577 -0.060732 -0.037634\n",
      "  0.046478  0.00435   0.025926 -0.114353  0.022489  0.007036 -0.038038\n",
      " -0.012441 -0.014459 -0.07476  -0.011494  0.040495 -0.05521   0.022157\n",
      "  0.03141   0.058666  0.085655  0.089744  0.06374   0.071349  0.101272\n",
      "  0.06275   0.007172  0.015388  0.012565  0.007443  0.009771 -0.002849\n",
      "  0.003162 -0.002294  0.007499  0.005444  0.003476]\n",
      "Epoch no\t14 \tLearning rate\t0.01 \n",
      "New weight[-0.05      0.020047  0.022276 -0.003503  0.063354  0.043598 -0.042593\n",
      " -0.051117 -0.012946  0.081334  0.088869  0.108167  0.032867 -0.025314\n",
      " -0.060035 -0.028607  0.00484   0.022692  0.010759  0.045394  0.026924\n",
      "  0.018416 -0.028555 -0.014497  0.059968 -0.016138 -0.060343 -0.034605\n",
      "  0.050286  0.000489  0.033307 -0.119413  0.027079  0.018455 -0.034462\n",
      " -0.011171 -0.015598 -0.082924 -0.005669  0.056618 -0.046244  0.034816\n",
      "  0.042969  0.063856  0.085531  0.091241  0.067544  0.07805   0.110331\n",
      "  0.067578  0.008438  0.017276  0.013821  0.008106  0.01103  -0.002359\n",
      "  0.003696 -0.002184  0.008304  0.006156  0.004124]\n",
      "Epoch no\t15 \tLearning rate\t0.01 \n",
      "New weight[-0.06      0.021294  0.023236 -0.003094  0.06762   0.04397  -0.048465\n",
      " -0.059246 -0.021147  0.07855   0.084537  0.106665  0.030207 -0.029351\n",
      " -0.068059 -0.035333 -0.007862  0.013318  0.007344  0.038144  0.019208\n",
      "  0.021082 -0.022934 -0.013877  0.063959 -0.003303 -0.045969 -0.022266\n",
      "  0.066226  0.010839  0.030849 -0.131096  0.026312  0.011244 -0.039754\n",
      " -0.008894 -0.018345 -0.091218 -0.010494  0.053369 -0.062505  0.024265\n",
      "  0.035817  0.065418  0.087118  0.091652  0.066227  0.080667  0.116124\n",
      "  0.071321  0.00836   0.01742   0.013628  0.007496  0.011667 -0.002908\n",
      "  0.003852 -0.002833  0.008544  0.005977  0.004271]\n",
      "Epoch no\t16 \tLearning rate\t0.01 \n",
      "New weight[-0.07      0.022248  0.024398 -0.004548  0.070921  0.047487 -0.051764\n",
      " -0.063214 -0.024826  0.076145  0.081964  0.1072    0.024168 -0.032565\n",
      " -0.068835 -0.032246 -0.001489  0.018     0.005902  0.043428  0.023244\n",
      "  0.013167 -0.031283 -0.014177  0.066733 -0.01234  -0.060053 -0.032202\n",
      "  0.066431  0.008938  0.028729 -0.144148  0.024383  0.014474 -0.037266\n",
      " -0.006818 -0.015477 -0.093351 -0.013337  0.055072 -0.065603  0.022496\n",
      "  0.0343    0.064596  0.08545   0.090216  0.066992  0.083918  0.120507\n",
      "  0.074058  0.009064  0.018786  0.014655  0.00831   0.012566 -0.002733\n",
      "  0.0042   -0.002751  0.009374  0.006793  0.004651]\n",
      "Epoch no\t17 \tLearning rate\t0.01 \n",
      "New weight[-0.07      0.024345  0.026736 -0.00247   0.078119  0.048897 -0.053413\n",
      " -0.065029 -0.01914   0.085788  0.087638  0.114534  0.026309 -0.033691\n",
      " -0.065306 -0.02631   0.00112   0.020621  0.003122  0.034945  0.024182\n",
      "  0.023042 -0.041053 -0.027788  0.066454 -0.011485 -0.061559 -0.040242\n",
      "  0.066445  0.003416  0.023242 -0.153327  0.030881  0.017387 -0.035525\n",
      " -0.003589 -0.013758 -0.093376 -0.005288  0.067422 -0.0674    0.013282\n",
      "  0.028052  0.05794   0.083664  0.089698  0.062671  0.083402  0.127205\n",
      "  0.077711  0.008531  0.019659  0.015218  0.008731  0.013805 -0.002458\n",
      "  0.005058 -0.00251   0.010194  0.007617  0.005339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no\t18 \tLearning rate\t0.01 \n",
      "New weight[-7.00000e-02  2.58060e-02  2.88320e-02 -3.17500e-03  8.14810e-02\n",
      "  5.26440e-02 -5.46060e-02 -6.95010e-02 -2.25090e-02  8.78840e-02\n",
      "  9.31700e-02  1.20293e-01  2.92510e-02 -3.41420e-02 -7.85540e-02\n",
      " -3.86890e-02 -1.69000e-03  2.08730e-02  1.06000e-04  2.68940e-02\n",
      "  1.43180e-02  2.25980e-02 -2.72810e-02 -1.60430e-02  7.18960e-02\n",
      " -6.96900e-03 -4.90040e-02 -2.63020e-02  7.65420e-02  1.20600e-03\n",
      "  1.89190e-02 -1.57934e-01  3.93720e-02  2.64030e-02 -3.29740e-02\n",
      "  4.59200e-03 -7.86200e-03 -9.21630e-02 -4.89000e-04  6.59840e-02\n",
      " -6.98410e-02  1.61940e-02  2.63670e-02  6.15010e-02  8.97660e-02\n",
      "  9.22810e-02  6.25360e-02  8.51590e-02  1.32455e-01  8.09390e-02\n",
      "  8.62500e-03  2.02340e-02  1.60760e-02  9.39700e-03  1.43710e-02\n",
      " -3.10000e-03  4.99200e-03 -3.29800e-03  9.85200e-03  7.72800e-03\n",
      "  5.52000e-03]\n",
      "Epoch no\t19 \tLearning rate\t0.01 \n",
      "New weight[-0.08      0.02612   0.029947 -0.003986  0.084201  0.05511  -0.054381\n",
      " -0.071159 -0.021661  0.089894  0.093875  0.12738   0.033671 -0.032279\n",
      " -0.077336 -0.029449  0.004653  0.02434  -0.001214  0.029712  0.018629\n",
      "  0.019693 -0.033889 -0.021298  0.072918 -0.011854 -0.063323 -0.035468\n",
      "  0.081699  0.006643  0.024107 -0.164443  0.037785  0.026336 -0.035107\n",
      "  0.003288 -0.009421 -0.099334 -0.009986  0.060945 -0.075331  0.009948\n",
      "  0.019586  0.057442  0.084436  0.085871  0.055108  0.08398   0.133705\n",
      "  0.081769  0.008408  0.021029  0.016721  0.009826  0.01514  -0.003064\n",
      "  0.005212 -0.003196  0.010525  0.008312  0.005975]\n",
      "Epoch no\t20 \tLearning rate\t0.01 \n",
      "New weight[-0.08      0.027297  0.030939 -0.004504  0.088091  0.056812 -0.057598\n",
      " -0.075051 -0.025834  0.088915  0.094786  0.130096  0.033567 -0.033734\n",
      " -0.083843 -0.035429  0.004393  0.025342 -0.005873  0.024504  0.011414\n",
      "  0.0159   -0.032169 -0.015458  0.079655 -0.011077 -0.060526 -0.029392\n",
      "  0.085777  0.00027   0.020259 -0.171254  0.044643  0.030256 -0.042548\n",
      "  0.001494 -0.006281 -0.09841  -0.005127  0.067409 -0.077039  0.015573\n",
      "  0.024628  0.060901  0.086481  0.089632  0.058509  0.08771   0.139324\n",
      "  0.084575  0.009115  0.022146  0.017927  0.010705  0.016138 -0.002957\n",
      "  0.00557  -0.003158  0.011228  0.009004  0.006524]\n",
      "Epoch no\t21 \tLearning rate\t0.01 \n",
      "New weight[-0.08      0.028855  0.032648 -0.003275  0.092981  0.056855 -0.059801\n",
      " -0.078107 -0.026619  0.089853  0.092562  0.131367  0.032949 -0.037786\n",
      " -0.086455 -0.036508  0.001487  0.02498  -0.007782  0.023737  0.008159\n",
      "  0.01808  -0.033744 -0.015582  0.084526 -0.010108 -0.058576 -0.025917\n",
      "  0.090153  0.002786  0.022743 -0.178509  0.043721  0.024794 -0.051768\n",
      " -0.000837 -0.006418 -0.100299 -0.002403  0.07828  -0.079795  0.009842\n",
      "  0.023087  0.060615  0.087795  0.089772  0.055544  0.087833  0.143675\n",
      "  0.087352  0.00847   0.022802  0.018512  0.010985  0.016986 -0.003007\n",
      "  0.00581  -0.003328  0.011722  0.009486  0.006936]\n",
      "Epoch no\t22 \tLearning rate\t0.01 \n",
      "New weight[-0.08      0.029583  0.033305 -0.003972  0.098058  0.060074 -0.061891\n",
      " -0.083271 -0.030077  0.091418  0.094879  0.133602  0.031379 -0.04132\n",
      " -0.093103 -0.038986  0.007375  0.039273  0.001209  0.028147  0.005062\n",
      "  0.016425 -0.037478 -0.017291  0.091945 -0.005984 -0.059726 -0.031086\n",
      "  0.089278 -0.001191  0.021033 -0.18213   0.050039  0.020836 -0.059742\n",
      " -0.008074 -0.011107 -0.108467 -0.012713  0.071058 -0.085799  0.010992\n",
      "  0.021278  0.056323  0.087495  0.08921   0.055434  0.089945  0.149217\n",
      "  0.090101  0.008867  0.024007  0.019833  0.011838  0.017764 -0.003238\n",
      "  0.006118 -0.003312  0.012286  0.009915  0.007115]\n",
      "Epoch no\t23 \tLearning rate\t0.01 \n",
      "New weight[-7.00000e-02  2.98550e-02  3.46970e-02 -3.40100e-03  1.02665e-01\n",
      "  6.30190e-02 -6.60810e-02 -9.00500e-02 -3.22290e-02  9.40460e-02\n",
      "  9.77990e-02  1.37434e-01  3.13520e-02 -4.16280e-02 -9.71750e-02\n",
      " -4.65010e-02  4.52000e-03  3.62900e-02 -4.58100e-03  2.06880e-02\n",
      "  8.59800e-03  2.24810e-02 -3.23010e-02 -1.24160e-02  9.58690e-02\n",
      " -4.81700e-03 -5.78380e-02 -3.36840e-02  9.30360e-02  1.70100e-03\n",
      "  2.55720e-02 -1.86193e-01  5.36980e-02  2.66390e-02 -4.85360e-02\n",
      "  3.76200e-03 -1.80000e-05 -1.03275e-01 -5.93900e-03  6.83190e-02\n",
      " -8.78140e-02  1.98540e-02  2.60470e-02  5.84710e-02  8.88730e-02\n",
      "  9.08210e-02  5.70840e-02  9.27520e-02  1.56400e-01  9.35670e-02\n",
      "  9.26000e-03  2.53260e-02  2.11770e-02  1.29000e-02  1.87370e-02\n",
      " -3.12200e-03  6.69500e-03 -3.54900e-03  1.25110e-02  1.06010e-02\n",
      "  7.58500e-03]\n",
      "Epoch no\t24 \tLearning rate\t0.01 \n",
      "New weight[-0.07      0.030948  0.035275 -0.002196  0.10794   0.064844 -0.070166\n",
      " -0.092601 -0.031521  0.099959  0.105631  0.145364  0.033    -0.030461\n",
      " -0.083809 -0.039605  0.002672  0.025361 -0.013402  0.018276  0.010297\n",
      "  0.018936 -0.043616 -0.028946  0.081956 -0.012113 -0.056636 -0.031308\n",
      "  0.092543  0.003305  0.034958 -0.186915  0.053831  0.03292  -0.040614\n",
      "  0.01118   0.003855 -0.103837 -0.001811  0.075174 -0.081639  0.027695\n",
      "  0.034192  0.067642  0.092734  0.092778  0.057951  0.097094  0.163786\n",
      "  0.097393  0.009441  0.027194  0.022042  0.013511  0.019555 -0.003167\n",
      "  0.007268 -0.003905  0.013484  0.011501  0.00807 ]\n",
      "Epoch no\t25 \tLearning rate\t0.01 \n",
      "New weight[-0.08      0.031694  0.036333 -0.003134  0.109585  0.065798 -0.071652\n",
      " -0.095333 -0.031213  0.099121  0.103627  0.148939  0.034242 -0.030937\n",
      " -0.082122 -0.027403  0.013263  0.032306 -0.00726   0.026341  0.01904\n",
      "  0.027435 -0.037823 -0.028999  0.081923 -0.014877 -0.064822 -0.039949\n",
      "  0.08671  -0.003442  0.034332 -0.199003  0.042193  0.027262 -0.044842\n",
      "  0.005496 -0.006119 -0.120142 -0.018047  0.065225 -0.088844  0.01922\n",
      "  0.027937  0.065174  0.087547  0.08559   0.049608  0.095122  0.163546\n",
      "  0.097703  0.009109  0.027817  0.022778  0.013766  0.020161 -0.003319\n",
      "  0.007546 -0.004049  0.01398   0.012072  0.008696]\n",
      "Epoch no\t26 \tLearning rate\t0.01 \n",
      "New weight[-0.07      0.031377  0.035954 -0.003629  0.111945  0.067346 -0.07452\n",
      " -0.101998 -0.037828  0.096205  0.103101  0.151061  0.037694 -0.031227\n",
      " -0.092603 -0.043177  0.003332  0.027936 -0.011244  0.015993  0.006012\n",
      "  0.01881  -0.035795 -0.023086  0.090973 -0.006869 -0.055545 -0.028096\n",
      "  0.092252 -0.003756  0.039941 -0.18968   0.055507  0.0359   -0.041026\n",
      "  0.008068 -0.001901 -0.120163 -0.016522  0.06052  -0.08892   0.028085\n",
      "  0.03277   0.072118  0.093646  0.087795  0.05076   0.098952  0.170521\n",
      "  0.100698  0.009414  0.029021  0.024047  0.014455  0.020566 -0.003802\n",
      "  0.007708 -0.004736  0.014005  0.01242   0.009132]\n",
      "Epoch no\t27 \tLearning rate\t0.01 \n",
      "New weight[-0.08      0.032108  0.037422 -0.004976  0.11338   0.069253 -0.07658\n",
      " -0.105026 -0.038801  0.097796  0.104739  0.156857  0.040829 -0.029656\n",
      " -0.093087 -0.036099  0.012435  0.035476 -0.005198  0.022019  0.008004\n",
      "  0.019507 -0.032918 -0.025041  0.091238 -0.005492 -0.060053 -0.032397\n",
      "  0.093434 -0.002318  0.04759  -0.196063  0.052779  0.039057 -0.041247\n",
      "  0.010701 -0.00149  -0.126753 -0.019835  0.063289 -0.085596  0.031158\n",
      "  0.031652  0.075264  0.098668  0.09017   0.046747  0.098592  0.172669\n",
      "  0.102261  0.009302  0.029636  0.024735  0.014904  0.021392 -0.00392\n",
      "  0.007903 -0.00491   0.014092  0.012855  0.00952 ]\n",
      "Epoch no\t28 \tLearning rate\t0.01 \n",
      "New weight[-8.00000e-02  3.27950e-02  3.87110e-02 -4.38500e-03  1.16213e-01\n",
      "  7.08400e-02 -7.80750e-02 -1.08436e-01 -4.35470e-02  9.12630e-02\n",
      "  1.01992e-01  1.53839e-01  3.75390e-02 -3.11320e-02 -1.01582e-01\n",
      " -4.51350e-02  6.99900e-03  2.91240e-02 -1.05030e-02  2.21790e-02\n",
      "  6.99600e-03  2.03510e-02 -2.24590e-02 -1.98030e-02  8.57430e-02\n",
      " -9.99500e-03 -5.88160e-02 -3.34430e-02  9.03850e-02 -3.05200e-03\n",
      "  4.71630e-02 -2.03846e-01  5.02500e-02  3.95340e-02 -4.46800e-02\n",
      "  1.12510e-02  1.38000e-04 -1.33590e-01 -2.62290e-02  6.00790e-02\n",
      " -8.95830e-02  2.67020e-02  2.67770e-02  7.49800e-02  9.83470e-02\n",
      "  8.87200e-02  4.22210e-02  9.87610e-02  1.76803e-01  1.04733e-01\n",
      "  9.45500e-03  3.04420e-02  2.56610e-02  1.52170e-02  2.23920e-02\n",
      " -4.03500e-03  7.94600e-03 -5.28200e-03  1.44250e-02  1.32760e-02\n",
      "  1.00710e-02]\n",
      "Epoch no\t29 \tLearning rate\t0.01 \n",
      "New weight[-0.08      0.034396  0.041034 -0.004433  0.11961   0.070176 -0.081155\n",
      " -0.111281 -0.043846  0.094539  0.103221  0.160531  0.042646 -0.02793\n",
      " -0.100975 -0.041071  0.013693  0.037994 -0.006432  0.023919  0.004014\n",
      "  0.017838 -0.03512  -0.031163  0.090601 -0.004784 -0.064908 -0.039788\n",
      "  0.096593 -0.001142  0.047321 -0.204166  0.056915  0.035832 -0.053431\n",
      "  0.006203  0.001172 -0.126085 -0.018893  0.072061 -0.089552  0.02343\n",
      "  0.023175  0.069616  0.101915  0.090298  0.039556  0.100774  0.181124\n",
      "  0.106857  0.008963  0.031319  0.026498  0.015828  0.023215 -0.003799\n",
      "  0.008336 -0.004972  0.014888  0.013987  0.010411]\n",
      "Epoch no\t30 \tLearning rate\t0.01 \n",
      "New weight[-0.08      0.034842  0.041884 -0.005921  0.122768  0.070179 -0.08492\n",
      " -0.115353 -0.047492  0.09595   0.106121  0.166814  0.045434 -0.028966\n",
      " -0.108356 -0.039386  0.020327  0.045943 -0.00278   0.027782  0.003173\n",
      "  0.023117 -0.027586 -0.029676  0.097677  0.002729 -0.063183 -0.037753\n",
      "  0.102514 -0.004012  0.047268 -0.210299  0.05828   0.035213 -0.058229\n",
      "  0.009181  0.006124 -0.1253   -0.02005   0.067569 -0.097038  0.024366\n",
      "  0.023648  0.070525  0.101221  0.087459  0.034808  0.100886  0.18332\n",
      "  0.107813  0.008545  0.032308  0.02744   0.016568  0.024144 -0.004048\n",
      "  0.008494 -0.004987  0.015109  0.014438  0.010796]\n",
      "Epoch no\t31 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.034519  0.041582 -0.007426  0.123782  0.071073 -0.085727\n",
      " -0.11673  -0.046324  0.095798  0.102861  0.167126  0.043151 -0.032377\n",
      " -0.106039 -0.032832  0.024413  0.042173 -0.007542  0.027495  0.006087\n",
      "  0.019697 -0.033926 -0.038529  0.091724 -0.005305 -0.072384 -0.043269\n",
      "  0.101462 -0.004491  0.051705 -0.214431  0.054809  0.034514 -0.059663\n",
      "  0.006996  0.001044 -0.132961 -0.022984  0.06678  -0.09464   0.020838\n",
      "  0.019164  0.068367  0.098746  0.087107  0.031376  0.101132  0.186177\n",
      "  0.109217  0.008563  0.033038  0.028052  0.016852  0.024627 -0.004285\n",
      "  0.008537 -0.005091  0.015594  0.014879  0.011309]\n",
      "Epoch no\t32 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.034749  0.041896 -0.007744  0.126862  0.073196 -0.089928\n",
      " -0.122326 -0.05259   0.093031  0.102412  0.167428  0.040649 -0.034717\n",
      " -0.11172  -0.036859  0.023402  0.042336 -0.002774  0.038764  0.016201\n",
      "  0.021213 -0.035199 -0.045951  0.085446 -0.010179 -0.073393 -0.044142\n",
      "  0.098915 -0.003942  0.052531 -0.219684  0.054351  0.029232 -0.066208\n",
      "  0.008936  0.006897 -0.129178 -0.017367  0.073323 -0.091906  0.020952\n",
      "  0.017142  0.065652  0.098999  0.0882    0.031192  0.105639  0.192127\n",
      "  0.112917  0.008844  0.033837  0.028798  0.017139  0.025533 -0.004512\n",
      "  0.008373 -0.005583  0.015961  0.015318  0.011667]\n",
      "Epoch no\t33 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.036152  0.042796 -0.007416  0.128368  0.073606 -0.088103\n",
      " -0.123304 -0.049959  0.093671  0.1011    0.172761  0.04499  -0.035893\n",
      " -0.114317 -0.037388  0.019173  0.033272 -0.015073  0.025102  0.009341\n",
      "  0.018187 -0.031955 -0.037669  0.098742  0.003561 -0.066624 -0.040798\n",
      "  0.10483  -0.001745  0.059262 -0.21632   0.062493  0.038766 -0.063731\n",
      "  0.007445  0.003736 -0.135343 -0.020031  0.075345 -0.091255  0.017383\n",
      "  0.017393  0.066673  0.096734  0.08367   0.0258    0.105269  0.19461\n",
      "  0.113823  0.008611  0.034265  0.029238  0.017583  0.026407 -0.004688\n",
      "  0.008571 -0.005573  0.016238  0.015702  0.012112]\n",
      "Epoch no\t34 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.038103  0.04499  -0.008403  0.132713  0.077938 -0.088186\n",
      " -0.125688 -0.053403  0.097619  0.10785   0.181232  0.057022 -0.021689\n",
      " -0.105063 -0.02935   0.028372  0.045512 -0.005594  0.03317   0.01392\n",
      "  0.022856 -0.033855 -0.049362  0.092388  0.001814 -0.068838 -0.043418\n",
      "  0.099853 -0.007327  0.055363 -0.221418  0.065659  0.0366   -0.073212\n",
      " -0.003279 -0.00241  -0.134351 -0.018388  0.072739 -0.096579  0.019543\n",
      "  0.020977  0.065273  0.094263  0.085446  0.025039  0.107702  0.20013\n",
      "  0.117347  0.008637  0.035375  0.029921  0.017613  0.027082 -0.004732\n",
      "  0.008555 -0.005953  0.016682  0.016232  0.012467]\n",
      "Epoch no\t35 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.039348  0.045996 -0.010251  0.135807  0.07835  -0.09234\n",
      " -0.130676 -0.056394  0.095425  0.104339  0.181444  0.05206  -0.0289\n",
      " -0.113442 -0.031845  0.027638  0.040368 -0.017621  0.026076  0.008731\n",
      "  0.015332 -0.038547 -0.048826  0.099817  0.00258  -0.068015 -0.038666\n",
      "  0.109802  0.000698  0.062964 -0.220057  0.072145  0.040343 -0.071759\n",
      " -0.000432  0.00922  -0.124495 -0.014514  0.072838 -0.102943  0.016417\n",
      "  0.019235  0.063745  0.095371  0.084945  0.022449  0.110132  0.204887\n",
      "  0.120416  0.008394  0.036645  0.03076   0.01829   0.027666 -0.005103\n",
      "  0.008588 -0.00606   0.016931  0.016976  0.012891]\n",
      "Epoch no\t36 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.040737  0.046852 -0.008866  0.140419  0.0823   -0.092022\n",
      " -0.130376 -0.055459  0.096263  0.105763  0.181836  0.047341 -0.02914\n",
      " -0.107398 -0.027681  0.024947  0.036101 -0.015827  0.035097  0.019077\n",
      "  0.021765 -0.02916  -0.037387  0.104247 -0.005046 -0.078068 -0.050574\n",
      "  0.097609 -0.002449  0.064314 -0.225749  0.06762   0.040595 -0.06307\n",
      "  0.006747  0.006086 -0.131649 -0.008435  0.082062 -0.096476  0.020496\n",
      "  0.019696  0.065116  0.095657  0.084239  0.018917  0.111037  0.207339\n",
      "  0.122537  0.008095  0.037968  0.031672  0.018829  0.028317 -0.005231\n",
      "  0.008659 -0.006586  0.017363  0.017592  0.013246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no\t37 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.041511  0.047853 -0.009498  0.143974  0.084162 -0.09407\n",
      " -0.134561 -0.061128  0.097628  0.111264  0.187811  0.055817 -0.019154\n",
      " -0.103971 -0.026474  0.026666  0.040123 -0.011795  0.034227  0.013728\n",
      "  0.020412 -0.035117 -0.051819  0.094483 -0.004049 -0.072691 -0.046193\n",
      "  0.092225 -0.012651  0.060666 -0.230186  0.06745   0.036575 -0.073672\n",
      " -0.00114   0.001995 -0.134316 -0.010568  0.076788 -0.100712  0.020255\n",
      "  0.024317  0.067452  0.092389  0.083661  0.017532  0.113485  0.212922\n",
      "  0.124985  0.008114  0.039088  0.032485  0.018957  0.029021 -0.005519\n",
      "  0.008555 -0.007148  0.01761   0.017842  0.013661]\n",
      "Epoch no\t38 \tLearning rate\t0.01 \n",
      "New weight[-9.00000e-02  4.14270e-02  4.79340e-02 -1.03650e-02  1.47087e-01\n",
      "  8.69130e-02 -9.42220e-02 -1.37083e-01 -6.16970e-02  9.85050e-02\n",
      "  1.10892e-01  1.88198e-01  5.47170e-02 -2.22470e-02 -1.06854e-01\n",
      " -2.51530e-02  3.16360e-02  4.25880e-02 -1.04930e-02  3.28470e-02\n",
      "  1.12060e-02  1.85060e-02 -3.37170e-02 -5.75250e-02  9.28740e-02\n",
      " -1.31000e-04 -6.97770e-02 -4.59080e-02  9.46230e-02 -1.31830e-02\n",
      "  6.46120e-02 -2.27763e-01  7.28900e-02  3.70240e-02 -7.74630e-02\n",
      " -2.84200e-03 -1.22700e-03 -1.35876e-01 -8.02100e-03  7.17580e-02\n",
      " -1.03692e-01  2.22970e-02  2.49380e-02  6.72160e-02  9.26430e-02\n",
      "  8.60260e-02  1.53120e-02  1.14556e-01  2.17127e-01  1.26750e-01\n",
      "  8.13800e-03  3.97130e-02  3.30290e-02  1.93140e-02  2.95850e-02\n",
      " -5.80700e-03  8.59100e-03 -7.26700e-03  1.79820e-02  1.81090e-02\n",
      "  1.41480e-02]\n",
      "Epoch no\t39 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.041633  0.048202 -0.010406  0.150728  0.092474 -0.09268\n",
      " -0.142687 -0.069678  0.095142  0.111259  0.187838  0.053737 -0.019838\n",
      " -0.107828 -0.031768  0.023551  0.042166 -0.008933  0.031823  0.008069\n",
      "  0.017896 -0.026849 -0.0489    0.09424  -0.001571 -0.068398 -0.04212\n",
      "  0.091697 -0.013462  0.066325 -0.231624  0.07272   0.039361 -0.077075\n",
      " -0.00685  -0.008193 -0.143997 -0.01238   0.068019 -0.107035  0.018542\n",
      "  0.022181  0.067795  0.089284  0.081733  0.009865  0.114266  0.2211\n",
      "  0.129599  0.008211  0.040787  0.034163  0.019501  0.029941 -0.006564\n",
      "  0.008452 -0.007988  0.01836   0.01847   0.014511]\n",
      "Epoch no\t40 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.041532  0.048577 -0.01165   0.151093  0.091733 -0.092238\n",
      " -0.144252 -0.069074  0.095081  0.109525  0.188243  0.052538 -0.027447\n",
      " -0.115849 -0.035708  0.01845   0.03402  -0.016092  0.025806  0.00671\n",
      "  0.016849 -0.023367 -0.0446    0.105704  0.014509 -0.059674 -0.039392\n",
      "  0.096277 -0.010669  0.075548 -0.228506  0.078414  0.047052 -0.070702\n",
      " -0.000343 -0.005197 -0.144666 -0.012133  0.07006  -0.104466  0.015368\n",
      "  0.020386  0.066513  0.090626  0.082888  0.007984  0.116052  0.223963\n",
      "  0.130538  0.007559  0.041102  0.034274  0.019818  0.03069  -0.006674\n",
      "  0.008603 -0.00812   0.018308  0.018648  0.014819]\n",
      "Epoch no\t41 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.043251  0.050185 -0.011801  0.151882  0.091922 -0.096326\n",
      " -0.146806 -0.071905  0.09442   0.109131  0.188937  0.050545 -0.028493\n",
      " -0.118002 -0.038684  0.014665  0.027092 -0.019278  0.0301    0.017111\n",
      "  0.022758 -0.018742 -0.041765  0.0974   -0.002451 -0.07511  -0.047271\n",
      "  0.090737 -0.010529  0.082917 -0.230126  0.078827  0.054345 -0.06632\n",
      "  0.00602   0.000505 -0.135949 -0.001298  0.079227 -0.093834  0.021351\n",
      "  0.022705  0.067248  0.092551  0.086652  0.010842  0.119865  0.22799\n",
      "  0.133452  0.007635  0.041849  0.035144  0.020071  0.031274 -0.006813\n",
      "  0.008542 -0.008707  0.018603  0.019006  0.01515 ]\n",
      "Epoch no\t42 \tLearning rate\t0.01 \n",
      "New weight[-0.1       0.044131  0.049372 -0.014061  0.153323  0.093733 -0.096516\n",
      " -0.146902 -0.073202  0.091328  0.106261  0.186159  0.045456 -0.025028\n",
      " -0.106038 -0.032136  0.018395  0.03265  -0.015215  0.03465   0.01499\n",
      "  0.01624  -0.018347 -0.036089  0.10527   0.003954 -0.071475 -0.045893\n",
      "  0.089013 -0.009116  0.080187 -0.240366  0.069498  0.044134 -0.078127\n",
      " -0.006431 -0.010475 -0.147632 -0.015343  0.072023 -0.097391  0.020787\n",
      "  0.019944  0.066392  0.092938  0.086941  0.007943  0.121603  0.232554\n",
      "  0.137119  0.007908  0.042794  0.035439  0.020276  0.03145  -0.00715\n",
      "  0.008514 -0.008922  0.019198  0.019663  0.015255]\n",
      "Epoch no\t43 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.045949  0.052092 -0.01333   0.157086  0.095836 -0.097819\n",
      " -0.150122 -0.076009  0.092873  0.111645  0.189371  0.044903 -0.028083\n",
      " -0.116188 -0.041401  0.010713  0.018967 -0.030197  0.023051  0.013893\n",
      "  0.019261 -0.015208 -0.039501  0.09778  -0.001209 -0.063857 -0.037253\n",
      "  0.095354 -0.009051  0.082676 -0.240484  0.078275  0.053081 -0.072438\n",
      "  0.007966  0.00892  -0.125835  0.006479  0.086211 -0.091199  0.025641\n",
      "  0.024668  0.073644  0.10315   0.096492  0.012086  0.123746  0.236253\n",
      "  0.138735  0.007945  0.043977  0.036809  0.020997  0.032366 -0.007466\n",
      "  0.008748 -0.009296  0.0195    0.020214  0.015971]\n",
      "Epoch no\t44 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.04853   0.053543 -0.012778  0.161011  0.097232 -0.098785\n",
      " -0.150419 -0.075802  0.096892  0.11644   0.19985   0.054722 -0.016892\n",
      " -0.107909 -0.035512  0.013378  0.025723 -0.022169  0.032098  0.022441\n",
      "  0.024375 -0.013583 -0.036336  0.090027 -0.014589 -0.075219 -0.040493\n",
      "  0.092457 -0.007011  0.090153 -0.240872  0.074592  0.052308 -0.077417\n",
      "  0.006052  0.002829 -0.132773 -0.002291  0.078172 -0.101997  0.015234\n",
      "  0.020846  0.071534  0.098601  0.092999  0.004311  0.121265  0.237781\n",
      "  0.140793  0.007389  0.044677  0.03765   0.021249  0.032773 -0.007583\n",
      "  0.008692 -0.009907  0.020036  0.020519  0.016258]\n",
      "Epoch no\t45 \tLearning rate\t0.01 \n",
      "New weight[-0.1       0.048761  0.053031 -0.013618  0.165244  0.101507 -0.097148\n",
      " -0.152209 -0.078035  0.090612  0.110045  0.197378  0.053662 -0.012934\n",
      " -0.09737  -0.029431  0.012862  0.02734  -0.022192  0.033392  0.018949\n",
      "  0.014012 -0.019088 -0.034042  0.098962 -0.003571 -0.06775  -0.039364\n",
      "  0.088229 -0.009116  0.08297  -0.254898  0.065197  0.045454 -0.077396\n",
      "  0.004235  0.000318 -0.140379 -0.014693  0.077408 -0.100325  0.017902\n",
      "  0.025995  0.077334  0.101724  0.092765 -0.000661  0.120919  0.240781\n",
      "  0.143048  0.007088  0.046061  0.037932  0.021597  0.033273 -0.007834\n",
      "  0.008898 -0.010099  0.020544  0.021248  0.01651 ]\n",
      "Epoch no\t46 \tLearning rate\t0.01 \n",
      "New weight[-0.1       0.048876  0.053218 -0.015263  0.167906  0.104302 -0.096078\n",
      " -0.153595 -0.07901   0.094358  0.116106  0.20155   0.054634 -0.014333\n",
      " -0.108037 -0.035986  0.012813  0.025097 -0.024978  0.033092  0.0181\n",
      "  0.017558 -0.012733 -0.035343  0.094185 -0.0089   -0.069078 -0.040136\n",
      "  0.088459 -0.012776  0.090775 -0.249437  0.064502  0.043394 -0.074149\n",
      "  0.014052  0.007873 -0.133528 -0.00203   0.080541 -0.1011    0.025438\n",
      "  0.024955  0.0794    0.107024  0.097243 -0.002496  0.122247  0.244046\n",
      "  0.144183  0.006924  0.047014  0.039087  0.022196  0.033521 -0.00821\n",
      "  0.009092 -0.010772  0.020433  0.021437  0.01692 ]\n",
      "Epoch no\t47 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.049792  0.053352 -0.016513  0.171232  0.106995 -0.094025\n",
      " -0.15259  -0.077694  0.096143  0.119427  0.206182  0.058291 -0.010568\n",
      " -0.112382 -0.041084  0.014507  0.02741  -0.024265  0.030371  0.010603\n",
      "  0.016401 -0.009431 -0.036123  0.095104 -0.007845 -0.063383 -0.03589\n",
      "  0.087866 -0.019181  0.092809 -0.244745  0.072066  0.048463 -0.079227\n",
      "  0.010182  0.003358 -0.139732 -0.002692  0.077118 -0.105679  0.027286\n",
      "  0.024841  0.081304  0.110208  0.102247 -0.001513  0.122427  0.248075\n",
      "  0.145984  0.007194  0.048196  0.040351  0.02309   0.03403  -0.008337\n",
      "  0.009244 -0.011481  0.02088   0.02193   0.017596]\n",
      "Epoch no\t48 \tLearning rate\t0.01 \n",
      "New weight[-0.1       0.051671  0.054127 -0.019033  0.171624  0.107889 -0.095935\n",
      " -0.154494 -0.077863  0.094246  0.114854  0.204886  0.055203 -0.011721\n",
      " -0.110805 -0.033292  0.021263  0.029783 -0.024964  0.031826  0.011197\n",
      "  0.017758 -0.010892 -0.038119  0.091061 -0.013407 -0.06953  -0.041184\n",
      "  0.083707 -0.022379  0.095499 -0.251005  0.065947  0.046989 -0.086587\n",
      "  0.007078  0.006062 -0.138152 -0.007479  0.075606 -0.10527   0.027621\n",
      "  0.027884  0.084841  0.114198  0.104619 -0.003319  0.122942  0.250942\n",
      "  0.148733  0.007044  0.048947  0.040689  0.023229  0.034483 -0.008549\n",
      "  0.00931  -0.011835  0.021313  0.022377  0.01804 ]\n",
      "Epoch no\t49 \tLearning rate\t0.01 \n",
      "New weight[-0.09      0.053149  0.055369 -0.018841  0.175626  0.110141 -0.096511\n",
      " -0.157248 -0.082718  0.095869  0.117512  0.207739  0.05438  -0.009284\n",
      " -0.112591 -0.037873  0.017167  0.026841 -0.024981  0.036898  0.016019\n",
      "  0.020112 -0.013272 -0.037406  0.089128 -0.018697 -0.070509 -0.034308\n",
      "  0.090716 -0.014678  0.107516 -0.246172  0.071174  0.0525   -0.08913\n",
      "  0.002794  0.006053 -0.137342 -0.003634  0.080072 -0.110136  0.02097\n",
      "  0.025595  0.0821    0.110049  0.102325 -0.004915  0.124415  0.255812\n",
      "  0.151952  0.007309  0.050118  0.042106  0.023563  0.035173 -0.00887\n",
      "  0.009058 -0.012558  0.021532  0.02265   0.018186]\n",
      "Epoch no\t50 \tLearning rate\t0.01 \n",
      "New weight[-0.1       0.055105  0.056442 -0.020765  0.177434  0.110142 -0.098785\n",
      " -0.157495 -0.080126  0.095395  0.113378  0.208851  0.050964 -0.010109\n",
      " -0.105731 -0.024259  0.027332  0.030035 -0.023866  0.036675  0.01639\n",
      "  0.019051 -0.017584 -0.0362    0.093254 -0.019619 -0.075552 -0.038637\n",
      "  0.092587 -0.012172  0.109875 -0.250134  0.065346  0.050008 -0.093209\n",
      "  0.00111   0.004598 -0.145099 -0.015445  0.071581 -0.116507  0.015576\n",
      "  0.018498  0.078255  0.108443  0.098788 -0.009885  0.123818  0.258875\n",
      "  0.154996  0.006904  0.050823  0.042482  0.023787  0.035527 -0.008713\n",
      "  0.00904  -0.012704  0.022005  0.023112  0.018469]\n",
      "Epoch no\t51 \tLearning rate\t0.01 \n",
      "New weight[-0.1       0.054341  0.056041 -0.021872  0.178516  0.109358 -0.098097\n",
      " -0.160771 -0.082709  0.092871  0.112084  0.206855  0.050911 -0.017978\n",
      " -0.115527 -0.033658  0.021013  0.02898  -0.024506  0.029527  0.006746\n",
      "  0.019006 -0.007066 -0.026979  0.102084 -0.010094 -0.068748 -0.0354\n",
      "  0.087206 -0.027696  0.101817 -0.253152  0.067629  0.048999 -0.092398\n",
      "  0.001438  0.002222 -0.146687 -0.010353  0.072197 -0.114876  0.017684\n",
      "  0.01733   0.078494  0.109751  0.09755  -0.01442   0.124226  0.260952\n",
      "  0.155746  0.006218  0.051675  0.043039  0.024085  0.035849 -0.00927\n",
      "  0.009247 -0.012849  0.022029  0.023203  0.01886 ]\n",
      "Epoch no\t52 \tLearning rate\t0.01 \n",
      "New weight[-0.1       0.055651  0.055993 -0.02158   0.181468  0.110383 -0.098692\n",
      " -0.163632 -0.083322  0.09712   0.116191  0.209114  0.05213  -0.019148\n",
      " -0.120771 -0.040306  0.013582  0.019849 -0.02427   0.037102  0.017598\n",
      "  0.020681 -0.013439 -0.029608  0.100297 -0.014653 -0.074792 -0.036828\n",
      "  0.087865 -0.029788  0.108068 -0.24794   0.073991  0.055804 -0.083804\n",
      "  0.014572  0.018553 -0.136179 -0.000752  0.083492 -0.109543  0.022556\n",
      "  0.022923  0.081161  0.112057  0.09749  -0.015836  0.126857  0.263624\n",
      "  0.156461  0.006072  0.052471  0.043881  0.024565  0.036389 -0.009542\n",
      "  0.009414 -0.013186  0.021861  0.023252  0.018946]\n",
      "Epoch no\t53 \tLearning rate\t0.01 \n",
      "New weight[-0.1       0.057699  0.05674  -0.021199  0.184295  0.109289 -0.097768\n",
      " -0.158182 -0.075378  0.104186  0.122547  0.217693  0.0576   -0.012905\n",
      " -0.107009 -0.026686  0.017759  0.014661 -0.027152  0.034557  0.010838\n",
      "  0.015893 -0.015317 -0.027929  0.099451 -0.020781 -0.085481 -0.042385\n",
      "  0.091026 -0.023736  0.115022 -0.247812  0.069219  0.048117 -0.091622\n",
      "  0.007792  0.00715  -0.145668  0.002932  0.094243 -0.100322  0.032781\n",
      "  0.026107  0.08316   0.113029  0.09848  -0.016969  0.12979   0.268068\n",
      "  0.158639  0.005739  0.053535  0.044883  0.025041  0.037163 -0.009206\n",
      "  0.009702 -0.013118  0.022626  0.023727  0.019304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no\t54 \tLearning rate\t0.01 \n",
      "New weight[-0.11      0.058744  0.057032 -0.021545  0.187766  0.111069 -0.1\n",
      " -0.162651 -0.079776  0.094026  0.115829  0.213936  0.055774 -0.009072\n",
      " -0.10026  -0.024254  0.014902  0.010473 -0.027979  0.032007  0.005143\n",
      "  0.007648 -0.020525 -0.029026  0.103655 -0.017393 -0.084732 -0.046337\n",
      "  0.087653 -0.026653  0.097565 -0.258819  0.067514  0.042806 -0.09681\n",
      "  0.003571 -0.000933 -0.159412 -0.007741  0.094098 -0.10295   0.032121\n",
      "  0.023559  0.077937  0.10944   0.092344 -0.02267   0.128861  0.269488\n",
      "  0.159203  0.005119  0.054318  0.045409  0.025332  0.037573 -0.009241\n",
      "  0.009908 -0.013476  0.022928  0.024378  0.019379]\n",
      "Epoch no\t55 \tLearning rate\t0.01 \n",
      "New weight[-0.11      0.05936   0.057607 -0.023332  0.190168  0.114789 -0.099477\n",
      " -0.163597 -0.083899  0.090619  0.112674  0.212635  0.054309 -0.005373\n",
      " -0.097096 -0.021598  0.020815  0.016748 -0.019087  0.041659  0.012034\n",
      "  0.007883 -0.019012 -0.029502  0.106207 -0.009413 -0.077958 -0.041889\n",
      "  0.09094  -0.021114  0.102531 -0.263749  0.067767  0.048702 -0.092435\n",
      "  0.007245  0.001574 -0.158181 -0.008     0.091824 -0.102289  0.033518\n",
      "  0.027182  0.081533  0.111445  0.094297 -0.0242    0.129356  0.272753\n",
      "  0.161481  0.005482  0.055026  0.045797  0.025458  0.03826  -0.009393\n",
      "  0.009672 -0.014047  0.023106  0.024922  0.019824]\n",
      "Epoch no\t56 \tLearning rate\t0.01 \n",
      "New weight[-0.1       0.060115  0.059115 -0.022828  0.1944    0.117093 -0.099449\n",
      " -0.164376 -0.083902  0.093352  0.11454   0.214825  0.0561   -0.005626\n",
      " -0.100928 -0.023127  0.022865  0.016529 -0.019017  0.041604  0.013372\n",
      "  0.012661 -0.016352 -0.032571  0.102548 -0.013974 -0.082204 -0.042112\n",
      "  0.094497 -0.019908  0.109775 -0.263316  0.065536  0.049272 -0.087157\n",
      "  0.016165  0.007626 -0.15518   0.000785  0.093772 -0.104246  0.03543\n",
      "  0.026264  0.082573  0.112746  0.095842 -0.025757  0.130408  0.27554\n",
      "  0.162643  0.005605  0.05614   0.047142  0.02616   0.038771 -0.009555\n",
      "  0.009782 -0.014297  0.023328  0.025366  0.020402]\n",
      "Epoch no\t57 \tLearning rate\t0.01 \n",
      "New weight[-0.11      0.061953  0.059494 -0.02401   0.196466  0.119933 -0.098759\n",
      " -0.165122 -0.083751  0.097536  0.118442  0.217128  0.053459 -0.007525\n",
      " -0.099721 -0.016806  0.023771  0.016885 -0.022877  0.033633  0.011631\n",
      "  0.015745 -0.005831 -0.025867  0.100214 -0.020583 -0.087815 -0.048316\n",
      "  0.08809  -0.021817  0.115954 -0.264181  0.063312  0.051325 -0.085274\n",
      "  0.015638 -0.001412 -0.164304 -0.006561  0.080546 -0.115339  0.022642\n",
      "  0.014715  0.077808  0.10626   0.091373 -0.028876  0.131379  0.279245\n",
      "  0.165701  0.00611   0.056939  0.047861  0.026415  0.039016 -0.01034\n",
      "  0.009597 -0.014918  0.023671  0.025499  0.020719]\n",
      "Epoch no\t58 \tLearning rate\t0.01 \n",
      "New weight[-0.11      0.061878  0.059133 -0.025066  0.198749  0.121009 -0.101147\n",
      " -0.169141 -0.087862  0.097031  0.120703  0.217696  0.05351  -0.007867\n",
      " -0.103458 -0.019741  0.022932  0.013833 -0.024337  0.031393  0.007356\n",
      "  0.016023 -0.00095  -0.029626  0.096169 -0.013226 -0.074551 -0.037463\n",
      "  0.090831 -0.026654  0.117844 -0.264264  0.064721  0.048416 -0.095588\n",
      "  0.010329 -0.004895 -0.163448 -0.001254  0.080892 -0.115645  0.026868\n",
      "  0.017712  0.080966  0.107315  0.092592 -0.032505  0.132609  0.282759\n",
      "  0.16674   0.006125  0.057767  0.048341  0.026662  0.039657 -0.010869\n",
      "  0.009667 -0.015186  0.023978  0.025704  0.021307]\n",
      "Epoch no\t59 \tLearning rate\t0.01 \n",
      "New weight[-0.1       0.063007  0.060644 -0.025226  0.202406  0.124555 -0.101203\n",
      " -0.170602 -0.092042  0.095622  0.120185  0.216822  0.05244  -0.005596\n",
      " -0.103345 -0.021543  0.021952  0.011072 -0.025427  0.03433   0.013199\n",
      "  0.020376  0.001118 -0.025701  0.096736 -0.01551  -0.072441 -0.033951\n",
      "  0.08834  -0.029085  0.120615 -0.270475  0.061366  0.056575 -0.087402\n",
      "  0.018399  0.001635 -0.15953   0.00552   0.086391 -0.11524   0.026162\n",
      "  0.018846  0.083005  0.106941  0.09243  -0.033972  0.132666  0.284716\n",
      "  0.168149  0.006574  0.059123  0.049649  0.027214  0.040195 -0.011154\n",
      "  0.009563 -0.015864  0.024116  0.026227  0.021854]\n",
      "Epoch no\t60 \tLearning rate\t0.01 \n",
      "New weight[-1.10000e-01  6.45130e-02  6.12230e-02 -2.80270e-02  2.02964e-01\n",
      "  1.24663e-01 -1.01468e-01 -1.68897e-01 -8.57110e-02  9.78020e-02\n",
      "  1.16125e-01  2.16495e-01  4.78120e-02 -9.36500e-03 -1.02105e-01\n",
      " -1.66880e-02  2.68740e-02  9.43700e-03 -3.06160e-02  2.90390e-02\n",
      "  7.01500e-03  1.20610e-02 -8.97100e-03 -2.71430e-02  1.07441e-01\n",
      " -8.81900e-03 -7.32950e-02 -3.70540e-02  9.18190e-02 -2.40060e-02\n",
      "  1.28845e-01 -2.65255e-01  6.59760e-02  5.99330e-02 -8.65740e-02\n",
      "  2.18700e-02  9.07800e-03 -1.57101e-01  3.00000e-05  8.29140e-02\n",
      " -1.10922e-01  3.04400e-02  1.85230e-02  8.40720e-02  1.15634e-01\n",
      "  9.92220e-02 -3.19710e-02  1.34990e-01  2.89615e-01  1.72027e-01\n",
      "  6.09600e-03  5.95400e-02  4.96240e-02  2.75180e-02  4.03410e-02\n",
      " -1.09330e-02  9.61700e-03 -1.61470e-02  2.44350e-02  2.67380e-02\n",
      "  2.21760e-02]\n",
      "Epoch no\t61 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.06464   0.060488 -0.02983   0.202423  0.122422 -0.104875\n",
      " -0.173116 -0.088723  0.094143  0.109957  0.211574  0.045003 -0.01579\n",
      " -0.106429 -0.018325  0.025293  0.008903 -0.029112  0.032081  0.009449\n",
      "  0.012669 -0.004112 -0.014842  0.116071 -0.002296 -0.071228 -0.033124\n",
      "  0.092063 -0.032352  0.126251 -0.270632  0.063116  0.054319 -0.099785\n",
      "  0.010264 -0.002008 -0.166618 -0.012     0.07529  -0.11304   0.031333\n",
      "  0.016303  0.079878  0.111637  0.093448 -0.040012  0.134466  0.291515\n",
      "  0.173274  0.005375  0.059791  0.0496    0.027367  0.040514 -0.011401\n",
      "  0.00975  -0.016479  0.02435   0.026715  0.022447]\n",
      "Epoch no\t62 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.065472  0.061403 -0.031893  0.203901  0.123727 -0.104312\n",
      " -0.172906 -0.090019  0.095461  0.114254  0.213842  0.044637 -0.014582\n",
      " -0.104439 -0.015328  0.029474  0.014319 -0.023594  0.036061  0.009656\n",
      "  0.016733 -0.003426 -0.017985  0.112905 -0.005942 -0.071441 -0.035342\n",
      "  0.084204 -0.048357  0.111722 -0.27831   0.067841  0.056501 -0.10198\n",
      "  0.010369  0.001889 -0.156264  0.002543  0.088783 -0.104257  0.033666\n",
      "  0.014979  0.078055  0.116549  0.099162 -0.037583  0.136793  0.294662\n",
      "  0.17595   0.005324  0.061233  0.050285  0.027686  0.041088 -0.011284\n",
      "  0.009644 -0.016657  0.024829  0.027041  0.022713]\n",
      "Epoch no\t63 \tLearning rate\t0.01 \n",
      "New weight[-0.11      0.06637   0.062543 -0.031508  0.205847  0.121627 -0.106587\n",
      " -0.172997 -0.089373  0.101884  0.120221  0.216989  0.045009 -0.0183\n",
      " -0.114104 -0.022519  0.025699  0.010894 -0.024995  0.03381   0.009772\n",
      "  0.022073 -0.002429 -0.023716  0.108368 -0.004635 -0.069172 -0.028343\n",
      "  0.092439 -0.047375  0.122341 -0.272379  0.071879  0.058389 -0.100563\n",
      "  0.01978   0.012305 -0.149245  0.009911  0.089235 -0.105807  0.030773\n",
      "  0.008672  0.076722  0.118467  0.100168 -0.039539  0.13837   0.298695\n",
      "  0.17814   0.005454  0.061962  0.051232  0.028209  0.041476 -0.011616\n",
      "  0.009766 -0.016871  0.024854  0.027143  0.023086]\n",
      "Epoch no\t64 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.067087  0.062219 -0.031801  0.207721  0.12356  -0.103667\n",
      " -0.1723   -0.089266  0.099634  0.113793  0.218622  0.049756 -0.014851\n",
      " -0.107419 -0.020109  0.02006   0.006409 -0.030209  0.024512 -0.004293\n",
      "  0.006627 -0.014009 -0.034416  0.101637 -0.010216 -0.077615 -0.037693\n",
      "  0.084634 -0.043981  0.127421 -0.274232  0.060517  0.049764 -0.101455\n",
      "  0.016173  0.003007 -0.162222 -0.001882  0.084323 -0.103684  0.032401\n",
      "  0.015952  0.084371  0.115748  0.099708 -0.037896  0.140415  0.303646\n",
      "  0.181222  0.005175  0.063081  0.051832  0.028702  0.041732 -0.012022\n",
      "  0.009803 -0.017181  0.025496  0.027455  0.023537]\n",
      "Epoch no\t65 \tLearning rate\t0.01 \n",
      "New weight[-0.11      0.067905  0.063857 -0.03135   0.21019   0.125734 -0.102121\n",
      " -0.170881 -0.090937  0.100698  0.112746  0.219941  0.052293 -0.012235\n",
      " -0.108332 -0.022629  0.018956  0.001279 -0.027291  0.03809   0.015474\n",
      "  0.018063 -0.013037 -0.031055  0.103685 -0.01299  -0.082279 -0.033081\n",
      "  0.094716 -0.040288  0.133343 -0.273592  0.069447  0.059776 -0.094932\n",
      "  0.019569  0.006266 -0.157026  0.012447  0.096662 -0.097408  0.035576\n",
      "  0.018369  0.08357   0.114843  0.09667  -0.0435    0.13917   0.305313\n",
      "  0.182079  0.00494   0.063999  0.052824  0.029075  0.042217 -0.011968\n",
      "  0.009849 -0.01745   0.025932  0.027747  0.024076]\n",
      "Epoch no\t66 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.070423  0.065532 -0.03213   0.211765  0.12606  -0.103737\n",
      " -0.172195 -0.089994  0.102359  0.111782  0.221747  0.050202 -0.010835\n",
      " -0.105644 -0.017743  0.022606  0.004983 -0.02707   0.037038  0.016665\n",
      "  0.015801 -0.016469 -0.027599  0.112764 -0.004942 -0.081773 -0.039029\n",
      "  0.095513 -0.034535  0.13658  -0.274287  0.073444  0.061446 -0.099899\n",
      "  0.016609  0.007323 -0.156044  0.002754  0.087215 -0.106226  0.028997\n",
      "  0.010658  0.076267  0.113159  0.093764 -0.046744  0.139064  0.307931\n",
      "  0.185167  0.004566  0.064463  0.052985  0.02929   0.042698 -0.011709\n",
      "  0.009848 -0.017547  0.026514  0.028298  0.02423 ]\n",
      "Epoch no\t67 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.072833  0.067343 -0.033519  0.214375  0.124068 -0.106474\n",
      " -0.171492 -0.087581  0.102573  0.111046  0.225495  0.049034 -0.007461\n",
      " -0.098361 -0.008127  0.029373  0.008182 -0.027188  0.033757  0.011451\n",
      "  0.013411 -0.025917 -0.036265  0.118001 -0.000679 -0.080926 -0.040302\n",
      "  0.103311 -0.027901  0.13405  -0.275547  0.078921  0.064717 -0.097563\n",
      "  0.021685  0.012155 -0.157275 -0.003649  0.084126 -0.110606  0.023655\n",
      "  0.001666  0.068487  0.111909  0.091556 -0.049576  0.138457  0.311297\n",
      "  0.188013  0.004194  0.065162  0.05355   0.029671  0.043321 -0.011198\n",
      "  0.010043 -0.017246  0.027106  0.029001  0.024412]\n",
      "Epoch no\t68 \tLearning rate\t0.01 \n",
      "New weight[-1.30000e-01  7.28260e-02  6.70040e-02 -3.38520e-02  2.15413e-01\n",
      "  1.24497e-01 -1.08462e-01 -1.74700e-01 -9.04480e-02  9.75470e-02\n",
      "  1.06134e-01  2.24488e-01  4.94070e-02 -8.91100e-03 -9.68870e-02\n",
      " -8.53100e-03  2.46960e-02  3.84400e-03 -2.68710e-02  3.92510e-02\n",
      "  1.60430e-02  1.02530e-02 -2.34000e-02 -3.10040e-02  1.20765e-01\n",
      "  4.50000e-05 -8.84410e-02 -4.77280e-02  9.77870e-02 -2.73280e-02\n",
      "  1.37278e-01 -2.81694e-01  6.90820e-02  5.27600e-02 -1.03282e-01\n",
      "  1.69260e-02  3.20100e-03 -1.65059e-01 -1.24500e-02  7.57910e-02\n",
      " -1.09762e-01  2.79510e-02  4.87200e-03  6.85340e-02  1.08028e-01\n",
      "  8.71940e-02 -5.55850e-02  1.39483e-01  3.12650e-01  1.89061e-01\n",
      "  3.59200e-03  6.55470e-02  5.38720e-02  2.98060e-02  4.36250e-02\n",
      " -1.15380e-02  1.01700e-02 -1.74520e-02  2.74830e-02  2.94610e-02\n",
      "  2.48520e-02]\n",
      "Epoch no\t69 \tLearning rate\t0.01 \n",
      "New weight[-0.13      0.073331  0.067134 -0.035308  0.215452  0.121793 -0.113343\n",
      " -0.178712 -0.095732  0.097061  0.108425  0.224232  0.045129 -0.008907\n",
      " -0.096409 -0.012654  0.018333 -0.00601  -0.029637  0.033039  0.009692\n",
      "  0.009149 -0.025212 -0.036373  0.115597 -0.003309 -0.090216 -0.052425\n",
      "  0.093746 -0.025544  0.136788 -0.282643  0.081093  0.066666 -0.096369\n",
      "  0.024488  0.008964 -0.16325  -0.006423  0.078812 -0.108373  0.029126\n",
      "  0.006647  0.071214  0.109428  0.084127 -0.05748   0.1403    0.314984\n",
      "  0.190604  0.003975  0.066219  0.054381  0.030132  0.044112 -0.011753\n",
      "  0.010028 -0.017863  0.027727  0.029702  0.02496 ]\n",
      "Epoch no\t70 \tLearning rate\t0.01 \n",
      "New weight[-0.13      0.073668  0.067521 -0.036764  0.215901  0.120621 -0.114358\n",
      " -0.180722 -0.095709  0.098018  0.111052  0.227252  0.047781 -0.01036\n",
      " -0.103087 -0.012579  0.026559 -0.001409 -0.025096  0.036328  0.011115\n",
      "  0.013555 -0.013182 -0.027779  0.120698  0.010245 -0.076859 -0.042935\n",
      "  0.098245 -0.032081  0.133687 -0.286387  0.081882  0.063665 -0.105122\n",
      "  0.019754  0.005829 -0.16474  -0.003828  0.082653 -0.104258  0.033894\n",
      "  0.006097  0.071266  0.111755  0.083426 -0.062179  0.140105  0.317478\n",
      "  0.19173   0.003745  0.066992  0.055059  0.030413  0.04474  -0.012022\n",
      "  0.010141 -0.018189  0.027671  0.029919  0.025571]\n",
      "Epoch no\t71 \tLearning rate\t0.01 \n",
      "New weight[-0.13      0.075693  0.069192 -0.038217  0.218301  0.118519 -0.118013\n",
      " -0.180997 -0.094344  0.097463  0.108661  0.229384  0.045323 -0.00777\n",
      " -0.095764 -0.003045  0.032729  0.00138  -0.024795  0.034745  0.009775\n",
      "  0.013811 -0.022337 -0.038201  0.122101  0.010508 -0.078631 -0.044818\n",
      "  0.106043 -0.02583   0.129619 -0.292015  0.080931  0.063339 -0.100403\n",
      "  0.030308  0.016605 -0.159074 -0.004178  0.08443  -0.10425   0.029417\n",
      " -0.001519  0.065677  0.113212  0.084433 -0.062298  0.141178  0.321629\n",
      "  0.194963  0.003414  0.067705  0.055545  0.030805  0.045337 -0.011537\n",
      "  0.010305 -0.017961  0.02816   0.030588  0.025755]\n",
      "Epoch no\t72 \tLearning rate\t0.01 \n",
      "New weight[-0.13      0.074803  0.068111 -0.038767  0.219908  0.118526 -0.119235\n",
      " -0.18487  -0.098648  0.098214  0.11266   0.229063  0.048142 -0.008071\n",
      " -0.101623 -0.01296   0.026808  0.001441 -0.020563  0.03691   0.010701\n",
      "  0.019382 -0.013395 -0.036651  0.119146  0.012033 -0.072018 -0.037092\n",
      "  0.099826 -0.042934  0.118805 -0.298942  0.078091  0.051812 -0.115171\n",
      "  0.016744 -0.001331 -0.167537 -0.003036  0.081725 -0.103875  0.030039\n",
      " -0.005206  0.062741  0.110811  0.08225  -0.065605  0.143035  0.323389\n",
      "  0.195575  0.00338   0.068752  0.056371  0.031226  0.045451 -0.012272\n",
      "  0.010475 -0.018575  0.028492  0.030726  0.026379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no\t73 \tLearning rate\t0.01 \n",
      "New weight[-1.20000e-01  7.46660e-02  6.92370e-02 -3.83040e-02  2.22554e-01\n",
      "  1.20550e-01 -1.21031e-01 -1.90888e-01 -1.05479e-01  8.98380e-02\n",
      "  1.03470e-01  2.22448e-01  4.40210e-02 -1.14640e-02 -1.05342e-01\n",
      " -2.07840e-02  2.13680e-02 -4.35200e-03 -2.47100e-02  3.66800e-02\n",
      "  9.57400e-03  1.16280e-02 -1.88490e-02 -3.21740e-02  1.29224e-01\n",
      "  1.95090e-02 -6.27750e-02 -3.02110e-02  1.00041e-01 -4.02770e-02\n",
      "  1.20379e-01 -2.98882e-01  9.07330e-02  6.74260e-02 -1.05364e-01\n",
      "  2.00440e-02 -6.80000e-05 -1.71023e-01 -6.35800e-03  8.20420e-02\n",
      " -9.94050e-02  3.55300e-02  2.71100e-03  6.63890e-02  1.11727e-01\n",
      "  8.02270e-02 -6.86880e-02  1.42612e-01  3.26035e-01  1.97325e-01\n",
      "  3.39600e-03  6.97220e-02  5.72590e-02  3.16900e-02  4.60620e-02\n",
      " -1.24310e-02  1.05410e-02 -1.88520e-02  2.88090e-02  3.13270e-02\n",
      "  2.68210e-02]\n",
      "Epoch no\t74 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.077412  0.070675 -0.039875  0.225587  0.12134  -0.118692\n",
      " -0.185537 -0.09668   0.097027  0.108064  0.230645  0.048395 -0.003942\n",
      " -0.088989 -0.002428  0.03471   0.008113 -0.017872  0.038313  0.006922\n",
      "  0.012599 -0.026321 -0.04567   0.123651  0.010456 -0.076706 -0.045304\n",
      "  0.095161 -0.039846  0.119783 -0.301445  0.090706  0.069069 -0.098418\n",
      "  0.030085  0.00843  -0.160827  0.003493  0.093235 -0.088735  0.039166\n",
      "  0.004261  0.070968  0.122747  0.089737 -0.064537  0.145376  0.330778\n",
      "  0.201905  0.0033    0.071039  0.057785  0.032375  0.04651  -0.011961\n",
      "  0.011002 -0.018634  0.029923  0.032269  0.027345]\n",
      "Epoch no\t75 \tLearning rate\t0.01 \n",
      "New weight[-1.30000e-01  7.65960e-02  7.06450e-02 -4.08620e-02  2.26303e-01\n",
      "  1.19679e-01 -1.19690e-01 -1.88920e-01 -1.00195e-01  9.71670e-02\n",
      "  1.03745e-01  2.26750e-01  4.59130e-02 -3.35100e-03 -9.60120e-02\n",
      " -1.47930e-02  2.24230e-02  8.20200e-03 -1.39860e-02  3.64270e-02\n",
      "  5.80100e-03  1.09370e-02 -2.76020e-02 -4.56070e-02  1.26161e-01\n",
      "  1.19560e-02 -7.82800e-02 -4.39680e-02  1.08753e-01 -2.32030e-02\n",
      "  1.30332e-01 -2.98662e-01  8.85340e-02  5.86270e-02 -1.08517e-01\n",
      "  1.69690e-02 -1.03310e-02 -1.71166e-01 -1.16000e-04  7.80110e-02\n",
      " -1.03005e-01  2.78530e-02 -6.92300e-03  6.12040e-02  1.16451e-01\n",
      "  8.63100e-02 -7.01260e-02  1.43956e-01  3.32073e-01  2.03134e-01\n",
      "  2.65200e-03  7.13170e-02  5.81220e-02  3.24200e-02  4.63960e-02\n",
      " -1.24350e-02  1.08870e-02 -1.90760e-02  2.98310e-02  3.21000e-02\n",
      "  2.70030e-02]\n",
      "Epoch no\t76 \tLearning rate\t0.01 \n",
      "New weight[-0.13      0.076664  0.07134  -0.04114   0.22886   0.121551 -0.119463\n",
      " -0.190459 -0.101709  0.101693  0.10858   0.22662   0.044733 -0.005824\n",
      " -0.099935 -0.015149  0.026346  0.007827 -0.01008   0.042161  0.012352\n",
      "  0.020755 -0.017943 -0.048018  0.114569  0.005527 -0.077707 -0.042087\n",
      "  0.105053 -0.029736  0.129479 -0.305866  0.081426  0.053418 -0.113037\n",
      "  0.017585 -0.011088 -0.16793   0.011576  0.086113 -0.096797  0.033196\n",
      " -0.005447  0.062925  0.116602  0.086942 -0.070184  0.146781  0.335575\n",
      "  0.204828  0.002801  0.072246  0.058849  0.032542  0.046966 -0.012952\n",
      "  0.010842 -0.01954   0.030092  0.032088  0.027426]\n",
      "Epoch no\t77 \tLearning rate\t0.01 \n",
      "New weight[-1.20000e-01  7.69260e-02  7.17740e-02 -4.05360e-02  2.31741e-01\n",
      "  1.23068e-01 -1.20012e-01 -1.92385e-01 -1.04158e-01  1.04305e-01\n",
      "  1.13692e-01  2.29519e-01  4.75460e-02 -6.13800e-03 -1.05641e-01\n",
      " -2.24540e-02  2.17140e-02  9.30000e-05 -1.50390e-02  4.26060e-02\n",
      "  1.41550e-02  2.16770e-02 -1.62580e-02 -4.84530e-02  1.09310e-01\n",
      "  1.44500e-03 -7.98320e-02 -4.29250e-02  1.01908e-01 -3.23590e-02\n",
      "  1.38208e-01 -3.01440e-01  8.79330e-02  5.90990e-02 -1.08997e-01\n",
      "  2.36510e-02 -4.99200e-03 -1.58756e-01  2.67440e-02  9.35250e-02\n",
      " -9.09300e-02  4.49060e-02  2.92700e-03  6.71080e-02  1.18473e-01\n",
      "  8.85700e-02 -7.19290e-02  1.47202e-01  3.36682e-01  2.04864e-01\n",
      "  2.82600e-03  7.32590e-02  6.00540e-02  3.31240e-02  4.76640e-02\n",
      " -1.30780e-02  1.09370e-02 -1.98390e-02  3.04170e-02  3.23670e-02\n",
      "  2.80190e-02]\n",
      "Epoch no\t78 \tLearning rate\t0.01 \n",
      "New weight[-0.11      0.077721  0.072954 -0.040519  0.235552  0.125596 -0.11895\n",
      " -0.193847 -0.108521  0.105099  0.118698  0.234666  0.051715 -0.001455\n",
      " -0.109231 -0.028914  0.015932 -0.002813 -0.018816  0.038121  0.010008\n",
      "  0.023583 -0.011722 -0.048959  0.106412  0.00198  -0.077271 -0.041198\n",
      "  0.098115 -0.036308  0.142708 -0.300256  0.09281   0.066628 -0.102563\n",
      "  0.028878 -0.000926 -0.158986  0.027751  0.090476 -0.096266  0.042248\n",
      "  0.004136  0.068537  0.115478  0.087138 -0.074723  0.146381  0.33967\n",
      "  0.206201  0.002696  0.074473  0.061334  0.033594  0.048389 -0.013356\n",
      "  0.010788 -0.020495  0.030545  0.032681  0.028493]\n",
      "Epoch no\t79 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.078949  0.073737 -0.042874  0.236656  0.127764 -0.120014\n",
      " -0.194034 -0.105963  0.106986  0.116114  0.233998  0.043675 -0.003936\n",
      " -0.103905 -0.016391  0.024864  0.001024 -0.012248  0.047015  0.013172\n",
      "  0.026008 -0.006586 -0.042585  0.111308 -0.003698 -0.083366 -0.053318\n",
      "  0.085206 -0.042779  0.146242 -0.302698  0.085653  0.063199 -0.108292\n",
      "  0.019593 -0.013986 -0.172531  0.017615  0.080195 -0.103007  0.040005\n",
      "  0.004064  0.06963   0.113906  0.088437 -0.072959  0.148435  0.342628\n",
      "  0.209166  0.002843  0.07519   0.061606  0.033803  0.048621 -0.01335\n",
      "  0.010622 -0.021106  0.030934  0.032736  0.028467]\n",
      "Epoch no\t80 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.080066  0.074805 -0.044183  0.239459  0.129914 -0.118958\n",
      " -0.193678 -0.104793  0.107609  0.116766  0.2353    0.040731 -0.005326\n",
      " -0.100711 -0.011259  0.026797 -0.003703 -0.019012  0.038024  0.005164\n",
      "  0.020749 -0.011412 -0.039924  0.122145  0.005157 -0.076718 -0.052072\n",
      "  0.083384 -0.043234  0.148663 -0.301789  0.091263  0.075609 -0.095819\n",
      "  0.025302 -0.012501 -0.176648  0.017955  0.081988 -0.100604  0.040624\n",
      "  0.002907  0.0711    0.115249  0.085376 -0.077444  0.147045  0.343762\n",
      "  0.210689  0.002653  0.076541  0.062513  0.034413  0.048939 -0.013465\n",
      "  0.010627 -0.021291  0.031369  0.033336  0.028972]\n",
      "Epoch no\t81 \tLearning rate\t0.01 \n",
      "New weight[-0.11      0.080686  0.075498 -0.045358  0.242689  0.131722 -0.116658\n",
      " -0.190849 -0.102734  0.109816  0.117069  0.237921  0.042478 -0.000852\n",
      " -0.097866 -0.011596  0.02874  -0.002851 -0.015326  0.041045  0.003945\n",
      "  0.025673 -0.012757 -0.044923  0.122974  0.005125 -0.073429 -0.046861\n",
      "  0.085663 -0.045865  0.150388 -0.29914   0.089445  0.068857 -0.104647\n",
      "  0.021188 -0.011733 -0.175334  0.02517   0.085807 -0.100114  0.041572\n",
      "  0.00478   0.074255  0.120929  0.093641 -0.070955  0.151484  0.349814\n",
      "  0.213902  0.002809  0.077774  0.063473  0.034999  0.049436 -0.013186\n",
      "  0.01052  -0.021821  0.031877  0.033772  0.029251]\n",
      "Epoch no\t82 \tLearning rate\t0.01 \n",
      "New weight[-1.20000e-01  8.07510e-02  7.50490e-02 -4.60420e-02  2.43401e-01\n",
      "  1.31453e-01 -1.17397e-01 -1.93259e-01 -1.06443e-01  1.05560e-01\n",
      "  1.12086e-01  2.31829e-01  3.78030e-02 -9.24100e-03 -1.08965e-01\n",
      " -2.47140e-02  1.55940e-02 -1.18820e-02 -1.94290e-02  3.55440e-02\n",
      "  2.00900e-03  2.36530e-02 -1.07090e-02 -4.47140e-02  1.18748e-01\n",
      "  6.60000e-05 -8.38040e-02 -4.91440e-02  8.66580e-02 -5.55090e-02\n",
      "  1.37295e-01 -3.09813e-01  8.55070e-02  5.84320e-02 -1.16655e-01\n",
      "  1.84460e-02 -1.05170e-02 -1.74725e-01  2.28150e-02  8.27310e-02\n",
      " -1.03644e-01  4.04260e-02  1.98700e-03  6.97660e-02  1.19255e-01\n",
      "  8.88420e-02 -7.87250e-02  1.50413e-01  3.51677e-01  2.14678e-01\n",
      "  2.29200e-03  7.78050e-02  6.35190e-02  3.51740e-02  4.96330e-02\n",
      " -1.36960e-02  1.05440e-02 -2.20060e-02  3.18370e-02  3.36790e-02\n",
      "  2.95440e-02]\n",
      "Epoch no\t83 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.082297  0.075701 -0.048656  0.244566  0.134549 -0.114763\n",
      " -0.188699 -0.100978  0.107924  0.113877  0.234809  0.035784 -0.006155\n",
      " -0.098629 -0.010735  0.026442 -0.012199 -0.018509  0.037364  0.000914\n",
      "  0.023052 -0.00544  -0.038856  0.123283  0.000965 -0.083664 -0.049258\n",
      "  0.088364 -0.049097  0.15259  -0.30235   0.083695  0.062075 -0.113739\n",
      "  0.020275 -0.014152 -0.186681  0.01682   0.081928 -0.097483  0.046542\n",
      "  0.003108  0.075489  0.125842  0.096052 -0.075707  0.153198  0.357816\n",
      "  0.218605  0.002471  0.078732  0.063855  0.035674  0.049788 -0.01367\n",
      "  0.010421 -0.022552  0.032058  0.034004  0.030002]\n",
      "Epoch no\t84 \tLearning rate\t0.01 \n",
      "New weight[-0.11      0.082949  0.076724 -0.049548  0.24795   0.138472 -0.113237\n",
      " -0.18853  -0.104748  0.107187  0.111595  0.232576  0.037986 -0.004522\n",
      " -0.10522  -0.019877  0.021907 -0.015073 -0.019973  0.037938  0.007485\n",
      "  0.028901 -0.006026 -0.041519  0.119839 -0.000862 -0.081711 -0.044145\n",
      "  0.094965 -0.047125  0.152047 -0.30826   0.086677  0.067492 -0.106241\n",
      "  0.029328 -0.003175 -0.175604  0.030191  0.088057 -0.098797  0.047887\n",
      "  0.005841  0.077243  0.127929  0.097789 -0.075443  0.154371  0.360709\n",
      "  0.220237  0.00289   0.079972  0.064941  0.036309  0.050361 -0.013702\n",
      "  0.010401 -0.022965  0.032243  0.034353  0.030489]\n",
      "Epoch no\t85 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.083556  0.078112 -0.050497  0.248402  0.137872 -0.116886\n",
      " -0.194201 -0.107612  0.106289  0.110281  0.234473  0.037262 -0.002822\n",
      " -0.106147 -0.016766  0.02573  -0.012612 -0.017378  0.041774  0.010312\n",
      "  0.02538  -0.013075 -0.048261  0.116005  0.002431 -0.078936 -0.047105\n",
      "  0.095261 -0.044607  0.155456 -0.313735  0.084498  0.071353 -0.103775\n",
      "  0.026378 -0.009977 -0.1814    0.022417  0.081576 -0.103036  0.044154\n",
      "  0.005562  0.07613   0.125591  0.094729 -0.081673  0.152773  0.360785\n",
      "  0.220916  0.002371  0.080625  0.065763  0.036533  0.050645 -0.013989\n",
      "  0.010466 -0.023389  0.032347  0.034512  0.03039 ]\n",
      "Epoch no\t86 \tLearning rate\t0.01 \n",
      "New weight[-1.20000e-01  8.33230e-02  7.82560e-02 -5.22710e-02  2.50113e-01\n",
      "  1.38442e-01 -1.14780e-01 -1.96732e-01 -1.10552e-01  1.03715e-01\n",
      "  1.08296e-01  2.34300e-01  4.06420e-02 -2.89200e-03 -1.09740e-01\n",
      " -1.90900e-02  2.86460e-02 -5.07900e-03 -1.16030e-02  3.77530e-02\n",
      " -1.87000e-04  2.09590e-02 -7.83500e-03 -4.31910e-02  1.20702e-01\n",
      "  8.69400e-03 -7.12350e-02 -4.28940e-02  8.75300e-02 -6.01550e-02\n",
      "  1.45410e-01 -3.22260e-01  7.95910e-02  6.87020e-02 -1.04796e-01\n",
      "  2.60840e-02 -6.73500e-03 -1.82987e-01  2.16610e-02  8.07150e-02\n",
      " -1.03521e-01  4.65840e-02  8.76200e-03  8.19150e-02  1.29371e-01\n",
      "  9.60010e-02 -8.26930e-02  1.52837e-01  3.64742e-01  2.22980e-01\n",
      "  2.19500e-03  8.19850e-02  6.66190e-02  3.68890e-02  5.10730e-02\n",
      " -1.44730e-02  1.05690e-02 -2.37700e-02  3.23180e-02  3.46570e-02\n",
      "  3.07570e-02]\n",
      "Epoch no\t87 \tLearning rate\t0.01 \n",
      "New weight[-1.20000e-01  8.45350e-02  7.81490e-02 -5.40600e-02  2.51782e-01\n",
      "  1.35478e-01 -1.15052e-01 -1.93834e-01 -1.07167e-01  1.06436e-01\n",
      "  1.11412e-01  2.38664e-01  3.85300e-02 -2.03000e-03 -1.04922e-01\n",
      " -1.20130e-02  3.13260e-02 -5.65000e-03 -1.08790e-02  3.75010e-02\n",
      " -2.71000e-04  2.21680e-02 -7.71500e-03 -4.44480e-02  1.25441e-01\n",
      "  1.79630e-02 -6.88280e-02 -4.00800e-02  1.02038e-01 -4.83910e-02\n",
      "  1.52912e-01 -3.19767e-01  7.81220e-02  6.40610e-02 -1.05431e-01\n",
      "  3.43590e-02  1.21800e-03 -1.82788e-01  1.56990e-02  7.62690e-02\n",
      " -1.11432e-01  3.77950e-02  2.80000e-05  7.68070e-02  1.26758e-01\n",
      "  9.08610e-02 -8.90370e-02  1.52111e-01  3.66476e-01  2.23597e-01\n",
      "  1.99400e-03  8.24420e-02  6.67430e-02  3.72160e-02  5.14740e-02\n",
      " -1.44270e-02  1.06220e-02 -2.35750e-02  3.26580e-02  3.49520e-02\n",
      "  3.09790e-02]\n",
      "Epoch no\t88 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.084837  0.078342 -0.053918  0.253652  0.136605 -0.115387\n",
      " -0.194449 -0.108913  0.106355  0.112565  0.240929  0.039098 -0.00298\n",
      " -0.11014  -0.019935  0.022429 -0.01756  -0.014347  0.042775  0.005576\n",
      "  0.018917 -0.013124 -0.047603  0.117572  0.008407 -0.079454 -0.045239\n",
      "  0.103314 -0.040921  0.167236 -0.315469  0.080708  0.069985 -0.097771\n",
      "  0.040813  0.001053 -0.18476   0.02052   0.083964 -0.102342  0.044592\n",
      "  0.002534  0.077825  0.127357  0.091706 -0.091963  0.152206  0.367349\n",
      "  0.223889  0.001494  0.082903  0.067554  0.03747   0.052089 -0.014488\n",
      "  0.010615 -0.024109  0.032868  0.03521   0.031319]\n",
      "Epoch no\t89 \tLearning rate\t0.01 \n",
      "New weight[-0.12      0.085898  0.079224 -0.054212  0.254602  0.138306 -0.1143\n",
      " -0.19484  -0.108747  0.108041  0.113397  0.242104  0.038907 -0.003483\n",
      " -0.110635 -0.018591  0.02337  -0.016663 -0.015907  0.039272  0.005331\n",
      "  0.017335 -0.005122 -0.0387    0.118945  0.008984 -0.080854 -0.050409\n",
      "  0.099829 -0.039044  0.169285 -0.32097   0.075521  0.064782 -0.101285\n",
      "  0.038018 -0.00242  -0.183062  0.022823  0.080782 -0.10566   0.044139\n",
      "  0.004819  0.077693  0.125259  0.093968 -0.090899  0.153206  0.370496\n",
      "  0.226057  0.001434  0.083788  0.068308  0.037835  0.052703 -0.014729\n",
      "  0.010327 -0.024716  0.032711  0.035077  0.031457]\n",
      "Epoch no\t90 \tLearning rate\t0.01 \n",
      "New weight[-0.13      0.086536  0.078008 -0.055947  0.256808  0.137211 -0.114551\n",
      " -0.1949   -0.109087  0.108999  0.114668  0.24313   0.035593 -0.002967\n",
      " -0.104984 -0.014335  0.020127 -0.017387 -0.011259  0.040028  0.000389\n",
      "  0.015686 -0.006595 -0.038584  0.121892  0.011924 -0.081786 -0.052036\n",
      "  0.1036   -0.03704   0.166231 -0.32419   0.070042  0.05611  -0.105825\n",
      "  0.037661 -0.002619 -0.19003   0.011887  0.075288 -0.11164   0.039804\n",
      " -0.00136   0.073774  0.121264  0.086161 -0.098551  0.15197   0.372082\n",
      "  0.226915  0.00078   0.084195  0.068441  0.037939  0.052931 -0.015082\n",
      "  0.01045  -0.024683  0.033139  0.035153  0.031394]\n",
      "Epoch no\t91 \tLearning rate\t0.01 \n",
      "New weight[-0.13      0.086939  0.078739 -0.056681  0.25938   0.13842  -0.11561\n",
      " -0.196906 -0.111842  0.112784  0.118255  0.24257   0.034873 -0.003128\n",
      " -0.109648 -0.017491  0.018735 -0.01723  -0.007562  0.04193   0.001675\n",
      "  0.022576 -0.003978 -0.046287  0.114838  0.011828 -0.08031  -0.048911\n",
      "  0.105298 -0.039963  0.169092 -0.324103  0.074128  0.056762 -0.109373\n",
      "  0.035799 -0.004441 -0.18785   0.019846  0.074818 -0.112179  0.042748\n",
      " -0.000595  0.073814  0.121005  0.084518 -0.103995  0.150921  0.37311\n",
      "  0.227641  0.000514  0.085004  0.068988  0.038235  0.053396 -0.015471\n",
      "  0.010429 -0.024772  0.033403  0.03531   0.031874]\n",
      "Epoch no\t92 \tLearning rate\t0.01 \n",
      "New weight[-1.30000e-01  8.75870e-02  7.97770e-02 -5.76400e-02  2.61541e-01\n",
      "  1.38767e-01 -1.16160e-01 -1.99152e-01 -1.12750e-01  1.16740e-01\n",
      "  1.22851e-01  2.44448e-01  3.57070e-02 -5.82600e-03 -1.17709e-01\n",
      " -2.24730e-02  1.66390e-02 -1.88900e-02 -1.01810e-02  3.77340e-02\n",
      "  2.88000e-04  2.71770e-02 -3.70700e-03 -5.17870e-02  1.09975e-01\n",
      "  1.15920e-02 -7.99530e-02 -4.66950e-02  1.08792e-01 -4.33940e-02\n",
      "  1.69454e-01 -3.26500e-01  7.87460e-02  6.09620e-02 -1.05942e-01\n",
      "  4.26930e-02  5.64800e-03 -1.74181e-01  3.21720e-02  7.95050e-02\n",
      " -1.08515e-01  4.65580e-02  2.80000e-04  7.49130e-02  1.25261e-01\n",
      "  8.54300e-02 -1.07984e-01  1.50896e-01  3.74221e-01  2.28664e-01\n",
      "  1.47000e-04  8.60720e-02  6.98140e-02  3.86760e-02  5.38380e-02\n",
      " -1.58630e-02  1.06090e-02 -2.47320e-02  3.37320e-02  3.55790e-02\n",
      "  3.23640e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no\t93 \tLearning rate\t0.01 \n",
      "New weight[-1.30000e-01  8.70880e-02  7.96410e-02 -5.89930e-02  2.64767e-01\n",
      "  1.41356e-01 -1.14297e-01 -2.00289e-01 -1.16499e-01  1.13887e-01\n",
      "  1.20926e-01  2.42078e-01  3.52830e-02 -5.40100e-03 -1.18103e-01\n",
      " -2.48260e-02  1.90320e-02 -1.18110e-02 -1.20100e-03  4.38010e-02\n",
      " -1.89000e-04  2.64040e-02 -4.01100e-03 -4.75890e-02  1.12780e-01\n",
      "  1.45480e-02 -7.39090e-02 -4.42520e-02  1.01641e-01 -5.76830e-02\n",
      "  1.56688e-01 -3.33312e-01  7.98780e-02  5.77930e-02 -1.14758e-01\n",
      "  3.47840e-02 -1.62200e-03 -1.80490e-01  2.68630e-02  7.52650e-02\n",
      " -1.09280e-01  5.05540e-02  3.53300e-03  7.52010e-02  1.25375e-01\n",
      "  8.55130e-02 -1.09008e-01  1.50843e-01  3.77071e-01  2.30814e-01\n",
      " -7.80000e-05  8.72340e-02  7.05360e-02  3.88680e-02  5.41510e-02\n",
      " -1.59890e-02  1.04800e-02 -2.53240e-02  3.40610e-02  3.56960e-02\n",
      "  3.24100e-02]\n",
      "Epoch no\t94 \tLearning rate\t0.01 \n",
      "New weight[-1.30000e-01  8.82640e-02  8.09460e-02 -6.05690e-02  2.66951e-01\n",
      "  1.44787e-01 -1.11564e-01 -2.00126e-01 -1.17472e-01  1.14315e-01\n",
      "  1.19922e-01  2.43088e-01  3.57740e-02 -5.92200e-03 -1.19550e-01\n",
      " -2.26650e-02  2.42220e-02 -9.66900e-03 -3.10700e-03  4.48940e-02\n",
      "  5.25400e-03  2.73520e-02 -6.11800e-03 -4.62660e-02  1.15345e-01\n",
      "  1.81150e-02 -7.21850e-02 -4.78600e-02  9.50670e-02 -6.18820e-02\n",
      "  1.54643e-01 -3.40745e-01  8.27410e-02  7.00470e-02 -1.05377e-01\n",
      "  3.74670e-02  1.55800e-03 -1.76265e-01  2.65730e-02  7.55440e-02\n",
      " -1.07774e-01  5.09660e-02  7.41200e-03  7.55620e-02  1.24766e-01\n",
      "  8.43540e-02 -1.10431e-01  1.49132e-01  3.76987e-01  2.31805e-01\n",
      " -1.44000e-04  8.84460e-02  7.13170e-02  3.91610e-02  5.46460e-02\n",
      " -1.63640e-02  1.03640e-02 -2.56940e-02  3.43160e-02  3.59230e-02\n",
      "  3.27280e-02]\n",
      "Epoch no\t95 \tLearning rate\t0.01 \n",
      "New weight[-1.30000e-01  8.80220e-02  7.97960e-02 -6.15000e-02  2.68897e-01\n",
      "  1.44232e-01 -1.12643e-01 -2.02345e-01 -1.18069e-01  1.16347e-01\n",
      "  1.18773e-01  2.42075e-01  3.48100e-02 -1.22840e-02 -1.26879e-01\n",
      " -2.77420e-02  2.02500e-02 -1.17240e-02 -4.27900e-03  3.90450e-02\n",
      " -3.84700e-03  2.31450e-02 -9.11900e-03 -4.87910e-02  1.18640e-01\n",
      "  2.26900e-02 -6.90040e-02 -3.98420e-02  1.02743e-01 -6.16290e-02\n",
      "  1.62357e-01 -3.35185e-01  8.37890e-02  6.38670e-02 -1.14266e-01\n",
      "  3.09900e-02 -4.73100e-03 -1.86781e-01  1.67910e-02  6.26160e-02\n",
      " -1.14033e-01  5.12800e-02  5.78300e-03  7.59870e-02  1.22048e-01\n",
      "  8.12980e-02 -1.14718e-01  1.48993e-01  3.80300e-01  2.33050e-01\n",
      " -3.19000e-04  8.89740e-02  7.19370e-02  3.95430e-02  5.47340e-02\n",
      " -1.70240e-02  1.04830e-02 -2.57180e-02  3.45910e-02  3.58920e-02\n",
      "  3.30880e-02]\n",
      "Epoch no\t96 \tLearning rate\t0.01 \n",
      "New weight[-0.13      0.089002  0.081394 -0.062312  0.270592  0.144501 -0.109219\n",
      " -0.200433 -0.11337   0.122457  0.122026  0.247053  0.03689  -0.013758\n",
      " -0.12531  -0.018629  0.030392 -0.006713 -0.002194  0.039966 -0.001656\n",
      "  0.024317 -0.00905  -0.04913   0.123414  0.026167 -0.075234 -0.046922\n",
      "  0.100545 -0.059677  0.172825 -0.32921   0.094259  0.076857 -0.102201\n",
      "  0.034579 -0.003061 -0.185897  0.021013  0.064001 -0.111544  0.048718\n",
      "  0.001717  0.073507  0.123357  0.078865 -0.12038   0.146615  0.380688\n",
      "  0.233998 -0.000555  0.089936  0.072725  0.039903  0.055209 -0.017266\n",
      "  0.01054  -0.025643  0.034901  0.03628   0.033619]\n",
      "Epoch no\t97 \tLearning rate\t0.01 \n",
      "New weight[-1.30000e-01  9.08850e-02  8.23400e-02 -6.28020e-02  2.72142e-01\n",
      "  1.46950e-01 -1.08398e-01 -1.99229e-01 -1.13798e-01  1.22630e-01\n",
      "  1.18351e-01  2.46515e-01  3.98500e-02 -1.20680e-02 -1.23914e-01\n",
      " -1.71060e-02  2.76340e-02 -1.10270e-02 -4.53600e-03  4.12770e-02\n",
      "  7.96200e-03  3.04380e-02 -2.95000e-03 -4.20200e-02  1.22771e-01\n",
      "  2.31270e-02 -7.71810e-02 -5.12890e-02  9.52730e-02 -6.15880e-02\n",
      "  1.72144e-01 -3.35656e-01  9.12430e-02  7.79670e-02 -9.91400e-02\n",
      "  3.60380e-02 -4.95800e-03 -1.82426e-01  2.84870e-02  7.43290e-02\n",
      " -1.07260e-01  4.65750e-02  3.00000e-05  7.18700e-02  1.22845e-01\n",
      "  8.16030e-02 -1.16371e-01  1.50209e-01  3.84734e-01  2.36708e-01\n",
      " -1.36000e-04  9.08550e-02  7.33570e-02  4.01650e-02  5.58820e-02\n",
      " -1.73870e-02  1.04220e-02 -2.61080e-02  3.51240e-02  3.64460e-02\n",
      "  3.40110e-02]\n",
      "Epoch no\t98 \tLearning rate\t0.01 \n",
      "New weight[-1.30000e-01  9.27210e-02  8.27890e-02 -6.33960e-02  2.74176e-01\n",
      "  1.48199e-01 -1.09152e-01 -1.97950e-01 -1.13240e-01  1.24583e-01\n",
      "  1.17195e-01  2.46676e-01  3.94210e-02 -9.32500e-03 -1.16129e-01\n",
      " -1.29590e-02  2.54060e-02 -1.59900e-02 -4.74600e-03  4.41670e-02\n",
      "  1.36360e-02  3.17990e-02 -5.16500e-03 -3.85230e-02  1.21681e-01\n",
      "  1.80080e-02 -7.78160e-02 -4.94580e-02  9.60360e-02 -6.02880e-02\n",
      "  1.75944e-01 -3.35462e-01  8.84310e-02  7.43210e-02 -1.07598e-01\n",
      "  3.16050e-02 -8.19300e-03 -1.78882e-01  3.38330e-02  8.17950e-02\n",
      " -1.02945e-01  4.63940e-02 -1.35900e-03  7.05270e-02  1.23652e-01\n",
      "  8.74180e-02 -1.10106e-01  1.55291e-01  3.90210e-01  2.40687e-01\n",
      "  3.77000e-04  9.17540e-02  7.38290e-02  4.02790e-02  5.64090e-02\n",
      " -1.71830e-02  1.01520e-02 -2.65720e-02  3.55870e-02  3.65790e-02\n",
      "  3.41530e-02]\n",
      "Epoch no\t99 \tLearning rate\t0.01 \n",
      "New weight[-0.14      0.092872  0.081996 -0.064632  0.275625  0.151031 -0.107239\n",
      " -0.198352 -0.115946  0.118593  0.113231  0.240683  0.032941 -0.01243\n",
      " -0.112928 -0.012462  0.021179 -0.018572 -0.008138  0.0367    0.006475\n",
      "  0.022643 -0.002842 -0.0362    0.116889  0.014174 -0.080122 -0.056094\n",
      "  0.0886   -0.063727  0.165843 -0.34582   0.080542  0.065254 -0.11491\n",
      "  0.028125 -0.012823 -0.184964  0.022271  0.070681 -0.111811  0.041693\n",
      " -0.004127  0.0697    0.122719  0.087185 -0.110502  0.156358  0.392845\n",
      "  0.242711  0.000798  0.092426  0.073908  0.040691  0.056579 -0.017596\n",
      "  0.009927 -0.026844  0.035906  0.036688  0.034254]\n",
      "Epoch no\t100 \tLearning rate\t0.01 \n",
      "New weight[-1.40000e-01  9.39410e-02  8.29700e-02 -6.59350e-02  2.76582e-01\n",
      "  1.48822e-01 -1.07605e-01 -1.99097e-01 -1.14636e-01  1.22556e-01\n",
      "  1.16472e-01  2.47233e-01  3.93410e-02 -1.04260e-02 -1.18926e-01\n",
      " -1.67630e-02  1.95970e-02 -1.95000e-02 -9.28000e-03  3.55110e-02\n",
      "  7.06400e-03  1.94030e-02 -1.04790e-02 -4.04960e-02  1.25652e-01\n",
      "  3.02110e-02 -6.91150e-02 -4.44200e-02  1.01342e-01 -5.77770e-02\n",
      "  1.78036e-01 -3.34921e-01  9.74860e-02  7.72930e-02 -1.12442e-01\n",
      "  2.97390e-02 -6.25100e-03 -1.78311e-01  2.10030e-02  7.05860e-02\n",
      " -1.09724e-01  3.99890e-02 -7.92900e-03  6.60150e-02  1.27054e-01\n",
      "  8.90130e-02 -1.16396e-01  1.55015e-01  3.94940e-01  2.43909e-01\n",
      "  9.60000e-05  9.26760e-02  7.41060e-02  4.09030e-02  5.70640e-02\n",
      " -1.76620e-02  1.02040e-02 -2.69650e-02  3.59120e-02  3.70340e-02\n",
      "  3.47490e-02]\n"
     ]
    }
   ],
   "source": [
    "#passing arguments as no of features\n",
    "perceptron = Neuron(60)\n",
    "#start train\n",
    "perceptron.train(training_inputs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted test output\n",
      " [0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      "Confusion Matrix\n",
      "[[17  2]\n",
      " [ 7 16]]\n",
      "Accuracy _score\t0.7857142857142857\n",
      "Accuracy using formula 0.7857142857142857\n",
      "Recall Score\t0.6956521739130435\n",
      "Recall using formula 0.6956521739130435\n",
      "Precision score\t0.8888888888888888\n",
      "precision using formula 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "#saving of predicted output to a variable\n",
    "a=mp.array([])\n",
    "x=a\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    output=perceptron.prediction(X_test[i])\n",
    "    x=mp.append(x,[output],axis=0)\n",
    "    \n",
    "print(\"predicted test output\\n\",x)\n",
    "\n",
    "#printing of accuracy metrics\n",
    "a=confusion_matrix(y_test,x)\n",
    "print(\"Confusion Matrix\\n\"+str(confusion_matrix(y_test, x)))\n",
    "tn=a[0][0]\n",
    "fp=a[0][1]\n",
    "fn=a[1][0]\n",
    "tp=a[1][1]\n",
    "\n",
    "\n",
    "print(\"Accuracy _score\\t\"+str(accuracy_score(y_test, x)))\n",
    "a=(tp+tn)/(tp+tn+fp+fn)\n",
    "print(\"Accuracy using formula\",a)\n",
    "print(\"Recall Score\\t\"+str(recall_score(y_test, x, average='binary')))\n",
    "r=tp/(tp+fn)\n",
    "print(\"Recall using formula\",r)\n",
    "print(\"Precision score\\t\"+str(precision_score(y_test, x, average='binary')))\n",
    "p=tp/(tp+fp)\n",
    "print(\"precision using formula\",p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  2]\n",
      " [ 7 16]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualization of confusion matrix\n",
    "labels = [0,1]\n",
    "cm = confusion_matrix(y_test, x,labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier\\n')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
